huList Content Comparisons


import sys
import os
import pandas as pd
import subprocess

def read_excel_file(excel_path, sheet_name):
    # Check if the file exists
    if not os.path.isfile(excel_path):
        print(f"Error: The file '{excel_path}' does not exist.")
        sys.exit(1)

    # Load the Excel file
    try:
        excel_file = pd.ExcelFile(excel_path)
    except Exception as e:
        print(f"An error occurred while reading the Excel file: {e}")
        sys.exit(1)
    
    # Check if the sheet name exists in the Excel file
    if sheet_name not in excel_file.sheet_names:
        print(f"Error: The sheet '{sheet_name}' does not exist in the Excel file.")
        sys.exit(1)

    # Load the specified sheet into a DataFrame
    try:
        df = pd.read_excel(excel_path, sheet_name=sheet_name)
        return df
    except Exception as e:
        print(f"An error occurred while reading the sheet: {e}")
        sys.exit(1)

def process_parameters_column(df):
    if 'Parameters' not in df.columns:
        print("Error: 'Parameters' column not found in the sheet.")
        sys.exit(1)
    
    parameters_list = df['Parameters'].dropna().apply(lambda x: x.split()).tolist()
    return parameters_list

def read_graph_paths(df):
    if 'GraphPath' not in df.columns:
        print("Error: 'GraphPath' column not found in the sheet.")
        sys.exit(1)
    
    graph_paths = df['GraphPath'].dropna().tolist()
    for path in graph_paths:
        try:
            result = subprocess.run(['cat', path], capture_output=True, text=True)
            if result.returncode == 0:
                print(f"Contents of {path}:\n{result.stdout}")
            else:
                print(f"Error reading {path}: {result.stderr}")
        except Exception as e:
            print(f"An error occurred while reading {path}: {e}")

def main():
    if len(sys.argv) != 3:
        print("Usage: python script.py <ExcelPath> <ExcelSheetName>")
        sys.exit(1)
    
    excel_path = sys.argv[1]
    excel_sheet_name = sys.argv[2]
    
    df = read_excel_file(excel_path, excel_sheet_name)
    
    # Process the Parameters column
    parameters_list = process_parameters_column(df)
    
    # Print the list of lists
    print("Parameters List:")
    print(parameters_list)
    
    # Read and print the contents of the GraphPath files
    read_graph_paths(df)

if __name__ == "__main__":
    main()

from pyspark.sql import SparkSession
from pyspark.sql.functions import monotonically_increasing_id

# Initialize Spark session
spark = SparkSession.builder.appName("DataFrameDuplicate").getOrCreate()

# Assuming you already have a DataFrame `df` with 3000 records
# Duplicate the DataFrame multiple times
def duplicate_dataframe(df, target_count):
    # Calculate how many times we need to duplicate the dataframe
    factor = target_count // df.count()
    
    # Create a new DataFrame by duplicating the original DataFrame
    duplicated_df = df
    for _ in range(factor - 1):
        duplicated_df = duplicated_df.union(df)
    
    # Calculate how many extra rows we need
    extra_rows = target_count - duplicated_df.count()
    
    # Add the extra rows if needed
    if extra_rows > 0:
        extra_df = df.limit(extra_rows)
        duplicated_df = duplicated_df.union(extra_df)
    
    return duplicated_df

# Example usage
target_count = 100000
duplicated_df = duplicate_dataframe(df, target_count)

# Add an index column if needed to differentiate between rows
duplicated_df = duplicated_df.withColumn("id", monotonically_increasing_id())

# Show the count of the final DataFrame
print(duplicated_df.count())

# Show some records from the final DataFrame
duplicated_df.show()


from pyspark.sql import SparkSession

# Initialize Spark Session
spark = SparkSession.builder.appName("ExpandDataFrame").getOrCreate()

# Assuming 'df' is your original DataFrame with 3,000 records
# Create a DataFrame that will be used to union with the original DataFrame
df_to_union = df.limit(100000)

# Initialize an empty DataFrame with the same schema as the original
expanded_df = spark.createDataFrame([], df.schema)

# Union the DataFrames until we reach the desired number of records
while expanded_df.count() < 100000:
    expanded_df = expanded_df.union(df_to_union)

# If the count exceeds 100,000, take only the first 100,000 records
if expanded_df.count() > 100000:
    expanded_df = expanded_df.limit(100000)

# Show the count to verify the number of records
print(expanded_df.count())


from pyspark.sql import SparkSession
import sys

def main(parquet_path):
    # Initialize Spark session
    spark = SparkSession.builder.appName("Parquet Reader").getOrCreate()

    # Read the Parquet files from the given path
    df = spark.read.parquet(parquet_path)

    # Get the count of records
    count = df.count()

    # Print the count
    print(f"Number of records: {count}")

    # Stop the Spark session
    spark.stop()

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: parquet_reader.py <path-to-parquet-files>")
        sys.exit(1)

    parquet_path = sys.argv[1]
    main(parquet_path)


def get_expected_value(*args):
    for condition in broadcast_conditions.value:
        if all(evaluate_condition(condition[f'column{i+1}'], args[i]) for i in range(len(args) - 1)):
            return condition['expected_value']
    return None

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

# Initialize Spark session
spark = SparkSession.builder.appName("PySpark UDF Example").getOrCreate()

# Read the CSV file with conditions
conditions_df = spark.read.option("header", "true").csv("/content/new_file.csv")

# Broadcast the conditions DataFrame
broadcast_conditions = spark.sparkContext.broadcast(conditions_df.collect())

def evaluate_condition(cond, value):
    if cond == 'any':
        return True
    condition = cond.replace('x', str(value))
    try:
        return eval(condition)
    except:
        return False

def get_expected_value(column1, column2, column3, column4):
    for condition in broadcast_conditions.value:
        if (
            evaluate_condition(condition['column1'], column1) and
            evaluate_condition(condition['column2'], column2) and
            evaluate_condition(condition['column3'], column3) and
            evaluate_condition(condition['column4'], column4)
        ):
            return condition['expected_value']
    return None

# Register UDF
get_expected_value_udf = udf(get_expected_value, StringType())

# Sample DataFrame
df = spark.createDataFrame([
    (-9997, 1, 'some_value', 5),
    (-9997, 3, 'any', 10),
    (-9997, -1, 'any', 7)
], ["column1", "column2", "column3", "column4"])

# Apply the UDF to create a new column
df_with_expected_value = df.withColumn("expected_value", get_expected_value_udf(df["column1"], df["column2"], df["column3"], df["column4"]))

# Show the resulting DataFrame
df_with_expected_value.show()



from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

# Initialize Spark session
spark = SparkSession.builder.appName("PySpark UDF Example").getOrCreate()

# Read the CSV file with conditions
conditions_df = spark.read.option("header", "true").csv("path_to_conditions_file.csv")

# Broadcast the conditions DataFrame
broadcast_conditions = spark.sparkContext.broadcast(conditions_df.collect())

# Function to get action based on the value
def get_action(value):
    for row in broadcast_conditions.value:
        if row['value'] == str(value):
            return row['action']
    return None

# Register UDF
get_action_udf = udf(get_action, StringType())

# Sample DataFrame
df = spark.createDataFrame([
    (52,),
    (51,),
    (60,)
], ["column_value"])

# Apply the UDF to create a new column
df_with_actions = df.withColumn("expected_flag", get_action_udf(df["column_value"]))

# Show the resulting DataFrame
df_with_actions.show()


column,condition,value
age,>,30
salary,<=,50000

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import BooleanType

# Initialize Spark session
spark = SparkSession.builder.appName("PySpark UDF Example").getOrCreate()

# Read the CSV file with conditions
conditions_df = spark.read.csv("path_to_conditions_file.csv", header=True, inferSchema=True)

# Convert conditions DataFrame to a list of dictionaries
conditions = conditions_df.collect()

# Function to apply conditions
def apply_conditions(*columns):
    column_names = ["age", "salary"]  # replace with your actual column names
    row = dict(zip(column_names, columns))
    
    for condition in conditions:
        col = condition['column']
        op = condition['condition']
        value = condition['value']
        
        if op == '>' and not row[col] > value:
            return False
        elif op == '>=' and not row[col] >= value:
            return False
        elif op == '<' and not row[col] < value:
            return False
        elif op == '<=' and not row[col] <= value:
            return False
        elif op == '==' and not row[col] == value:
            return False
        elif op == '!=' and not row[col] != value:
            return False
    
    return True

# Register UDF
apply_conditions_udf = udf(apply_conditions, BooleanType())

# Sample DataFrame to apply conditions
df = spark.createDataFrame([
    (25, 30000),
    (35, 60000),
    (40, 40000)
], ["age", "salary"])

# Apply UDF to DataFrame
filtered_df = df.filter(apply_conditions_udf(*[df[col] for col in df.columns]))

filtered_df.show()


# Splitting the multiline string on newline characters
lines = multiline_string.split('\n')

# Creating a DataFrame from the split lines
df = pd.DataFrame(lines, columns=['Text'])

#!/bin/bash

# Initialize variables
LOGFILE="logfile.log"
IMP_INFO="imp_info.txt"
REPORT="report.html"

# Create or truncate the log file
> "$LOGFILE"

# Create the HTML report header
echo "<html><body><table>" > "$REPORT"
echo "<tr><th>Iteration no.</th><th>Log filename</th><th>Total Processed</th><th>Total Success</th><th>Total Error</th><th>Beeline Output</th></tr>" >> "$REPORT"

# Loop 10 times
for i in {1..10}; do
    # Run ksh script and log output with timestamp
    {
        echo "Iteration $i: $(date)"
        # Your ksh script commands go here
        # Example:
        # command1 arg1
        # command2 arg2
    } 2>&1 | tee -a "$LOGFILE"

    # Extract relevant information from log file (you'll need to adjust this)
    processed=$(grep "Total Processed" "$LOGFILE" | tail -n 1)
    success=$(grep "Total Success" "$LOGFILE" | tail -n 1)
    error=$(grep "Total Error" "$LOGFILE" | tail -n 1)

    # Run Hive query using Beeline and append results
    beeline_output=$(beeline -u jdbc:hive2://your_hive_server:port -e "your_query")

    # Append data to the HTML report
    echo "<tr><td>$i</td><td>$LOGFILE</td><td>$processed</td><td>$success</td><td>$error</td><td>$beeline_output</td></tr>" >> "$REPORT"
done

# Add ksh script lines to the HTML report
echo "<p>Lines from ksh script:</p>" >> "$REPORT"
sed -n '7,9p' "$LOGFILE" >> "$REPORT"

# Close the HTML table
echo "</table></body></html>" >> "$REPORT"

# Display the last 20 lines from the log file
tail -n 20 "$LOGFILE" > "$IMP_INFO"




#!/bin/bash

# Define the ksh script path
ksh_script_path="/path/to/your/ksh_script.ksh"

# Define the log file path
log_file="logfile.log"

# Define the iterations
iterations=10

# Function to run the ksh script, store output, and display on terminal
run_ksh_script() {
  timestamp=$(date +"%Y-%m-%d_%H-%M-%S")
  ksh_output=$(ksh "$ksh_script_path")

  # Write output with timestamp to log file
  echo "$timestamp: $ksh_output" >> "$log_file"

  # Display output on terminal
  echo "$ksh_output"

  # Extract lines 7, 8, and 9 from ksh_output for HTML report
  ksh_report_lines=$(echo "$ksh_output" | sed -n '7,9p')
}

# Create the HTML report header
echo "<html>
<body>
<table>
  <tr>
    <th>Iteration No.</th>
    <th>Log Filename</th>
    <th>Total Processed</th>
    <th>Total Success</th>
    <th>Total Error</th>
    <th>Beeline Query Output</th>
    <th>Ksh Script Lines (7, 8, 9)</th>
  </tr>" > report.html

# Loop for the iterations
for ((i=1; i<=$iterations; i++)); do
  # Run the ksh script and capture output
  run_ksh_script

  # Extract relevant information from log file
  total_processed=$(grep -c "Total Processed" "$log_file")
  total_success=$(grep -c "Total Success" "$log_file")
  total_error=$(grep -c "Total Error" "$log_file")

  # Get last 20 lines of log file and store in imp_info.txt
  tail -n 20 "$log_file" > imp_info.txt

  # Run beeline query and append output to imp_info.txt
  # Replace 'your_beeline_query' with your actual query
  beeline -e "your_beeline_query" >> imp_info.txt

  # Extract beeline query output from imp_info.txt
  beeline_output=$(tail -n 10 imp_info.txt)

  # Add information to HTML report table
  echo "<tr>
    <td>$i</td>
    <td>$log_file</td>
    <td>$total_processed</td>
    <td>$total_success</td>
    <td>$total_error</td>
    <td>$beeline_output</td>
    <td>$ksh_report_lines</td>
  </tr>" >> report.html

  # Clear imp_info.txt for next iteration
  > imp_info.txt
done

# Close the HTML report
echo "</table>
</body>
</html>" >> report.html

echo "HTML report generated: report.html"


#!/bin/bash

# Define variables
KSH_SCRIPT="your_script.ksh"
LOG_PREFIX="script_output"
IMP_INFO_FILE="imp_info.txt"
HTML_REPORT="report.html"
HIVE_QUERY="YOUR_HIVE_QUERY"
HIVE_SERVER="your_hive_server"
HIVE_USER="your_username"
HIVE_PASSWORD="your_password"

# Initialize the HTML report
echo "<html><head><title>Script Execution Report</title></head><body>" > $HTML_REPORT
echo "<h1>Script Execution Report</h1>" >> $HTML_REPORT

# Extract lines 7, 8, 9 from the ksh script and add to HTML report
echo "<h2>Lines 7, 8, 9 from KSH script</h2>" >> $HTML_REPORT
echo "<pre>" >> $HTML_REPORT
sed -n '7,9p' $KSH_SCRIPT >> $HTML_REPORT
echo "</pre>" >> $HTML_REPORT

# Start the table in the HTML report
echo "<table border='1'>" >> $HTML_REPORT
echo "<tr><th>Iteration No.</th><th>Log Filename</th><th>Total Processed</th><th>Total Success</th><th>Total Error</th><th>Beeline Query Output</th></tr>" >> $HTML_REPORT

# Clear the imp_info.txt file before starting the loop
> $IMP_INFO_FILE

# Loop to run the process 10 times
for i in {1..10}; do
    TIMESTAMP=$(date +'%Y-%m-%d_%H-%M-%S')
    LOG_FILE="${LOG_PREFIX}_${TIMESTAMP}.log"

    # Run the ksh script and store the output in a log file with a timestamp
    ksh $KSH_SCRIPT | tee $LOG_FILE

    # Get the last 20 lines from the log file and store in imp_info.txt
    tail -n 20 $LOG_FILE >> $IMP_INFO_FILE

    # Run the Hive query using beeline and append the result to imp_info.txt
    BEELINE_OUTPUT=$(beeline -u jdbc:hive2://$HIVE_SERVER:10000 -n $HIVE_USER -p $HIVE_PASSWORD -e "$HIVE_QUERY")
    echo "$BEELINE_OUTPUT" >> $IMP_INFO_FILE

    # Extract relevant information from the log file
    TOTAL_PROCESSED=$(grep "Total Processed" $LOG_FILE | awk -F: '{print $2}' | xargs)
    TOTAL_SUCCESS=$(grep "Total Success" $LOG_FILE | awk -F: '{print $2}' | xargs)
    TOTAL_ERROR=$(grep "Total Error" $LOG_FILE | awk -F: '{print $2}' | xargs)

    # Append the iteration result to the HTML report
    echo "<tr><td>$i</td><td>$LOG_FILE</td><td>$TOTAL_PROCESSED</td><td>$TOTAL_SUCCESS</td><td>$TOTAL_ERROR</td><td><pre>$BEELINE_OUTPUT</pre></td></tr>" >> $HTML_REPORT
done

# Finalize the HTML report
echo "</table></body></html>" >> $HTML_REPORT




#!/bin/bash

# Define the name of the ksh script to run
KSH_SCRIPT="your_ksh_script.ksh"

# Define the log file name prefix
LOG_FILE_PREFIX="output_"

# Define the Hive query to execute using Beeline
HIVE_QUERY="SELECT * FROM your_table LIMIT 10;"

# Initialize counters
total_processed=0
total_success=0
total_error=0

# Array to hold all log filenames
declare -a log_files

# Function to generate HTML report
generate_html_report() {
    local iteration=$1
    local log_file=$2
    local beeline_output=$3
    local success_lines=$(grep -c "Success" $log_file)
    local error_lines=$(grep -c "Error" $log_file)

    echo "<tr><td>$iteration</td><td>$log_file</td><td>$total_processed</td><td>$success_lines</td><td>$error_lines</td><td>$beeline_output</td></tr>" >> report.html
}

for i in {1..10}
do
    # Generate unique log file name for each iteration
    LOG_FILE="${LOG_FILE_PREFIX}${i}.log"

    # Run the ksh script and redirect both stdout and stderr to the log file
    eval "$KSH_SCRIPT" > >(tee -a $LOG_FILE) 2>&1

    # Display the last 20 lines of the log file on the terminal
    tail -n 20 $LOG_FILE

    # Extract the last 20 lines from the log file and store them in imp_info.txt
    tail -n 20 $LOG_FILE > imp_info.txt

    # Execute the Hive query using Beeline and append the result to imp_info.txt
    beeline -u jdbc:hive2://localhost:10000/default -e "$HIVE_QUERY" >> imp_info.txt

    # Update counters based on log file content
    total_processed=$((total_processed+20))
    total_success=$(($total_success+$success_lines))
    total_error=$(($total_error+$error_lines))

    # Add log file name to array
    log_files+=("$LOG_FILE")

    # Generate HTML report for this iteration
    generate_html_report $i $LOG_FILE $(cat imp_info.txt | grep -v "^$" | head -n 20)

    echo "Iteration $i completed."
done

# Start HTML report generation
echo "<html><head><title>All Iterations Report</title></head><body><table border='1'>" >> report.html
echo "<tr><th>Iteration No.</th><th>Log Filename</th><th>Total Processed</th><th>Total Success</th><th>Total Error</th><th>Beeline Output</th></tr>" >> report.html

# Populate HTML report with all iterations
for log_file in "${log_files[@]}"; do
    # Find corresponding iteration number
    iteration_number=$(basename "$log_file".log | tr -d '_')
    # Append the report row for this iteration
    generate_html_report $iteration_number $log_file $(cat imp_info.txt | grep -v "^$" | head -n 20)
done

# Close HTML report
echo "</table></body></html>" >> report.html

echo "HTML report generated."

# Clean up temporary files
rm imp_info.txt

#!/bin/bash

# Define the command to run
COMMAND="your_command_here"

# Define the Hive query to run
HIVE_QUERY="your_hive_query_here"

# Number of repetitions
REPEAT=10

for i in $(seq 1 $REPEAT); do
    echo "Iteration: $i"

    # Create a log file name with a current timestamp
    LOG_FILE="log_$(date +%Y%m%d_%H%M%S).log"

    # Run the command and save the output to the log file and terminal
    $COMMAND | tee -a "$LOG_FILE"

    # Extract the last 30 lines from the log file into a new txt file
    tail -n 30 "$LOG_FILE" > "last_30_lines.txt"

    # Run the Hive query and save the result to a txt file
    hive -e "$HIVE_QUERY" > "output.txt"
done



import os
import shutil
from datetime import datetime

def copy_files_in_time_range(start_time_str, end_time_str, source_path, target_path):
    # Convert start and end times to datetime objects
    start_time = datetime.strptime(start_time_str, "%Y-%m-%d %H:%M:%S")
    end_time = datetime.strptime(end_time_str, "%Y-%m-%d %H:%M:%S")
    
    # Ensure target directory exists
    if not os.path.exists(target_path):
        os.makedirs(target_path)
    
    # Iterate over files in the source directory
    for filename in os.listdir(source_path):
        file_path = os.path.join(source_path, filename)
        
        # Check if it is a file
        if os.path.isfile(file_path):
            # Get the creation time of the file
            creation_time = datetime.fromtimestamp(os.path.getctime(file_path))
            
            # Check if the file's creation time is within the specified range
            if start_time <= creation_time <= end_time:
                # Copy the file to the target directory
                shutil.copy(file_path, target_path)
                print(f"Copied: {filename}")

# Example usage
start_time = "2023-01-01 00:00:00"
end_time = "2023-12-31 23:59:59"
source_path = "/path/to/source"
target_path = "/path/to/target"

copy_files_in_time_range(start_time, end_time, source_path, target_path)



Subject: Apology for Access Card Violation

Dear [Recipient's Name],

I hope this message finds you well.

I am writing to address the recent access card violation incident. On [date of incident], I entered the basement area to collect a mouse, unaware that it was a restricted zone.

I sincerely apologize for this oversight. I understand the importance of adhering to access protocols and ensuring security within the premises. Moving forward, I will be more vigilant and ensure that I am fully aware of all access restrictions to prevent any future violations.

Thank you for your understanding, and please accept my apologies once again.

Best regards,

[Your Name]  
[Your Job Title]  
[Your Contact Information]

#!/bin/bash

# Specify the database name
dbName="new_db"

# Temporary file to store table names
tempFile=$(mktemp)

# Use Hive to list tables in the specified database and filter them using grep
hive -e "USE $dbName; SHOW TABLES;" | grep -E '^*' > $tempFile

# Loop through each table listed in the temporary file
while IFS= read -r tableName; do
    echo "Processing table: $tableName"
    
    # Example operation: Describe the table
    # Replace this with your desired operation
    hive -e "DESCRIBE $tableName;"
done < $tempFile

# Clean up: Remove the temporary file
rm $tempFile

echo "Finished processing tables."



#!/bin/bash

# Initialize variables
htmlOutput="output.html"
tempFile=$(mktemp)

# Start HTML content
echo "<html><body>" > $htmlOutput
echo "<h2>HDFS Size Report</h2>" >> $htmlOutput
echo "<table border='1'>" >> $htmlOutput
echo "<tr><th>Table Name</th><th>HDFS Location</th><th>Size</th></tr>" >> $htmlOutput

# Assuming the list of tables is stored in a file called 'table_list.txt'
# Replace 'table_list.txt' with the path to your actual file containing table names
while IFS= read -r tableName; do
    echo "$tableName" >> $tempFile
done < table_list.txt

# Read the temporary file and process each table
while IFS= read -r tableName; do
    # Get the HDFS location of the table
    hdfsLocation=$(hive -e "DESCRIBE FORMATTED $tableName;" | grep "Location:" | awk '{print $NF}')

    # Check if HDFS location exists
    if [! -z "$hdfsLocation" ]; then
        # Get the size of the HDFS location
        size=$(hadoop fs -du -h "$hdfsLocation" | tail -n 1 | awk '{print $1}')
        
        # Append the result to the HTML file
        echo "<tr><td>$tableName</td><td>$hdfsLocation</td><td>$size</td></tr>" >> $htmlOutput
    else
        echo "No HDFS location found for table $tableName."
    fi
done < $tempFile

# End HTML content
echo "</table></body></html>" >> $htmlOutput

# Clean up: Remove the temporary file
rm $tempFile

# Display completion message
echo "Report saved to $htmlOutput"



import pandas as pd
import subprocess  # Import the subprocess module

# Define execution_stops_decision if needed
execution_stops_decision = False  # Example value, adjust as necessary

# Step 1: Read the 'run_sequence' sheet from the Excel file
file_path = 'C:\\Users\\pravisha\\Downloads\\pandas_practise.xlsx'
df = pd.read_excel(file_path, sheet_name='run_sequence')

# Initialize an empty DataFrame to store the results
results_df = pd.DataFrame(columns=['graph_path', 'results', 'last_line'])

def run_command_and_get_output(command):
    """
    Runs a command and returns the last line of the output and the return code.

    Parameters:
    - command: A list of strings representing the command to run.

    Returns:
    - A tuple containing the last line of the output and the return code.
    """
    # Run the command and capture the output and stderr
    output, stderr = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

    # Decode the output to UTF-8 and strip leading/trailing whitespace
    output_decoded = output.decode('utf-8').strip()

    # Get the last line of the output
    last_line = output_decoded.split('\n')[-1]

    # Return the last line and the return code
    return last_line, output.returncode

# Example usage within your existing code
for index, row in df.iterrows():
    command = ["air", "sandbox", "run"]
    graph_path = row['sandbox_name'] + row['path']
    command.append(graph_path)

    for col in df.columns:
        if col in ['variable1', 'variable2', 'variable3']:
            if pd.notna(row[col]):
                variable_name, value = row[col].split()
                command.append(variable_name)
                command.append(value)

    if 'Stop_Execution_If_Fails' in df.columns:
        execution_stops_decision = row['Stop_Execution_If_Fails'].lower()

    # Run the command and get the last line and return code
    last_line, return_code = run_command_and_get_output(command)

    # Check if the command was successful
    if return_code!= 0 or "command successful" not in last_line:
        results = 'Failed'
    else:
        results = 'Passed'

    # Append the results to the results_df DataFrame
    results_df = results_df.append({'graph_path': graph_path, 'results': results, 'last_line': last_line}, ignore_index=True)

    # Check if execution should stop
    if execution_stops_decision == 'yes' and return_code!= 0:
        break

# Generate an HTML table from the results_df DataFrame
html_table = results_df.to_html(index=False)

# Prepare the email content
email_subject = "Command Execution Results"
email_body = f"""
<html>
<head>
<title>Command Execution Results</title>
</head>
<body>
<h2>Command Execution Results</h2>
{html_table}
</body>
</html>
"""

# Prepare the email headers
email_headers = "From: sender@example.com\r\nTo: recipient@example.com\r\nSubject: {}\r\n".format(email_subject)

# Send the email using sendmail
import os
os.system(f'sendmail -t < /dev/stdin', input=email_headers + email_body)

print("Email has been sent.")


import pandas as pd
import subprocess  # Import the subprocess module

# Define execution_stops_decision if needed
execution_stops_decision = False  # Example value, adjust as necessary

# Step 1: Read the 'run_sequence' sheet from the Excel file
file_path = 'C:\\Users\\pravisha\\Downloads\\pandas_practise.xlsx'
df = pd.read_excel(file_path, sheet_name='run_sequence')

# Initialize an empty DataFrame to store the results
results_df = pd.DataFrame(columns=['last_line', 'results'])


def run_command_and_get_output(command):
    """
    Runs a command and returns the last line of the output and the return code.

    Parameters:
    - command: A list of strings representing the command to run.

    Returns:
    - A tuple containing the last line of the output and the return code.
    """
    # Run the command and capture the output and stderr
    output, stderr = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

    # Decode the output to UTF-8 and strip leading/trailing whitespace
    output_decoded = output.decode('utf-8').strip()

    # Get the last line of the output
    last_line = output_decoded.split('\n')[-1]

    # Return the last line and the return code
    return last_line, output.returncode

# Example usage within your existing code
for index, row in df.iterrows():
    command = ["air", "sandbox", "run"]
    graph_path = row['sandbox_name'] + row['path']
    command.append(graph_path)

    for col in df.columns:
        if col in ['variable1', 'variable2', 'variable3']:
            if pd.notna(row[col]):
                variable_name, value = row[col].split()
                command.append(variable_name)
                command.append(value)

    if 'Stop_Execution_If_Fails' in df.columns:
        execution_stops_decision = row['Stop_Execution_If_Fails'].lower

    # Run the command and get the last line and return code
    last_line, return_code = run_command_and_get_output(command)

    # Check if the command was successful
    if return_code != 0 or "command successful" not in last_line:
        results = 'Failed'
    else:
        results = 'Passed'

    # Append the results to the results_df DataFrame
    results_df = results_df.append({'last_line': last_line, 'results': results}, ignore_index=True)

    # Check if execution should stop
    if execution_stops_decision == 'yes' and return_code!= 0:
        break

    # Write the results_df DataFrame to an Excel file
    results_df.to_excel(file_path, sheet_name='run_sequence', mode='a', index=False)

    # Print the command, execution_stops_decision, and the last line of the output
    print("Results have been written to the Excel file.")
    print(command)
    print(execution_stops_decision)
    print(last_line)




To include the `evidences` variable from each test case in your custom HTML report, you need to modify the `pytest_runtest_makereport` hook in your `conftest.py` to capture and store this variable. Since the `evidences` variable is specific to each test case, you'll need to ensure it's accessible within the scope of the hook. One way to achieve this is by attaching the `evidences` variable to the test item (`item`) object, which is then accessible in the `pytest_runtest_makereport` hook.

Here's how you can modify the `conftest.py` to include the `evidences` variable:

```python
# conftest.py

import pytest
from datetime import datetime

# This list will store test results and evidence
test_results = []

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    outcome = yield
    rep = outcome.get_result()
    if rep.when == "call":
        # Assuming the test case sets an 'evidences' attribute on the item
        evidence = getattr(item, 'evidences', 'No evidence provided')
        # Collect test results and evidence here
        test_results.append({
            'class_name': item.parent.name,
            'test_name': item.name,
            'outcome': rep.outcome,
            'evidence': evidence
        })

def pytest_sessionfinish(session, exitstatus):
    # Generate the HTML report here
    generate_html_report(test_results)

def generate_html_report(results):
    # This function generates the HTML report
    html_content = """
    <html>
    <head>
        <title>Test Report</title>
    </head>
    <body>
        <h1>Test Report</h1>
        <p>Generated on: {}</p>
        <table border="1">
            <tr>
                <th>Test Class Name</th>
                <th>Test Case Name</th>
                <th>Outcome</th>
                <th>Evidence</th>
            </tr>
    """.format(datetime.now())

    for result in results:
        html_content += """
            <tr>
                <td>{}</td>
                <td>{}</td>
                <td>{}</td>
                <td>{}</td>
            </tr>
        """.format(result['class_name'], result['test_name'], result['outcome'], result['evidence'])

    html_content += """
        </table>
    </body>
    </html>
    """

    # Write the HTML content to a file
    with open('test_report.html', 'w') as f:
        f.write(html_content)
```

In your test cases, you need to set the `evidences` attribute on the test item (`item`). You can do this by using the `pytest.mark.parametrize` decorator or by setting the attribute directly in your test functions. Here's an example of how to set the `evidences` attribute in a test function:

```python
def test_example(item):
    # Your test code here
    item.evidences = "Your collected evidence here"
    # Your assertions here
```

Remember, the `evidences` variable should be set within each test case before the test outcome is determined, so it's captured and included in the report. This approach allows you to include any relevant information from each test case in your custom HTML report, providing a comprehensive overview of your test results and the evidence supporting those results.

Citations:
[1] https://stackoverflow.com/questions/29597972/collecting-and-reporting-pytest-results
[2] https://www.youtube.com/watch?v=DpfLjB84EQU
[3] https://pytest-html.readthedocs.io/en/latest/user_guide.html
[4] https://docs.pytest.org/en/latest/reference/plugin_list.html
[5] https://github.com/pytest-dev/pytest-html/issues/508
[6] https://docs.pytest.org/en/7.1.x/reference/plugin_list.html
[7] https://github.com/pytest-dev/pytest-html/issues/176
[8] https://pytest-with-eric.com/pytest-best-practices/pytest-plugins/
[9] https://docs.pytest.org/en/6.2.x/usage.html



‚öîÔ∏è Arena (battle)
‚öîÔ∏è Arena (side-by-side)
üí¨ Direct Chat
üëÄ Vision Direct Chat
üèÜ Leaderboard
‚ÑπÔ∏è About Us
‚öîÔ∏è  LMSYS Chatbot Arena: Benchmarking LLMs in the Wild
| Blog | GitHub | Paper | Dataset | Twitter | Discord |

üìú Rules
Ask any question to two anonymous models (e.g., ChatGPT, Claude, Llama) and vote for the better one!
You can continue chatting until you identify a winner.
Vote won't be counted if model identity is revealed during conversation.
üèÜ LMSYS Arena 
Leaderboard
We've collected 700,000+ human votes to compute an LLM Elo leaderboard. Find out who is the ü•áLLM Champion!

üëá Chat now!

üîç Expand to see the descriptions of 41 models
‚ñº
Model A

I am a ETL tester. Create a test suite with pytest.I cant use pytest-html plugin. Create a custom html report in conftest.py. In which we will have test class name, test case name, outcome pass or fail and evidence in table format. My Usual tests include checking graph file exists, run graph file, check data in source and target hive table using pyspark.



Here's an example of how you can create a test suite with Pytest and a custom HTML report in conftest.py:

conftest.py


import pytest
from datetime import datetime
from jinja2 import Environment, FileSystemLoader

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    # execute all other hooks to obtain the report object
    outcome = yield
    rep = outcome.get_result()

    # we only look at actual failing test calls, not setup/teardown
    if rep.when == "call" and rep.failed:
        mode = "a" if os.path.exists("failures") else "w"
        with open("failures", mode) as f:
            # let's also access the test function object
            if "tmpdir" in item.fixturenames:
                extra = " (%s)" % item.funcargs["tmpdir"]
            else:
                extra = ""
            f.write(rep.nodeid + extra + "\n")

# Create a custom HTML report
@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_terminal_summary(terminalreporter, exitstatus, config):
    if exitstatus == 0:
        return

    # Get the test results
    results = terminalreporter.stats.get('passed', []) + terminalreporter.stats.get('failed', [])

    # Create a Jinja2 environment
    env = Environment(loader=FileSystemLoader(''))

    # Render the HTML template
    template = env.get_template('report_template.html')
    html = template.render(results=results)

    # Write the HTML report to a file
    with open('report.html', 'w') as f:
        f.write(html)

# Define the HTML template
REPORT_TEMPLATE = '''
<!DOCTYPE html>
<html>
  <head>
    <title>ETL Test Report</title>
    <style>
      table {
        border-collapse: collapse;
      }
      th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;
      }
    </style>
  </head>
  <body>
    <h1>ETL Test Report</h1>
    <table>
      <tr>
        <th>Test Class</th>
        <th>Test Case</th>
        <th>Outcome</th>
        <th>Evidence</th>
      </tr>
      {% for result in results %}
      <tr>
        <td>{{ result.nodeid.split('::')[-2] }}</td>
        <td>{{ result.nodeid.split('::')[-1] }}</td>
        <td>{{ 'Passed' if result.passed else 'Failed' }}</td>
        <td>{{ result.longreprtext if not result.passed else '' }}</td>
      </tr>
      {% endfor %}
    </table>
  </body>
</html>
'''

# Create a report_template.html file
with open('report_template.html', 'w') as f:
    f.write(REPORT_TEMPLATE)
test_etl.py


import pytest
from pyspark.sql import SparkSession

@pytest.fixture
def spark():
    return SparkSession.builder.appName("ETL Test").getOrCreate()

class TestETL:
    def test_graph_file_exists(self, spark):
        # Check if graph file exists
        assert os.path.exists('path/to/graph/file')

    def test_run_graph_file(self, spark):
        # Run the graph file
        spark.sql('RUN GRAPH path/to/graph/file')

    def test_source_data(self, spark):
        # Check data in source hive table
        df = spark.sql('SELECT * FROM source_table')
        assert df.count() > 0

    def test_target_data(self, spark):
        # Check data in target hive table
        df = spark.sql('SELECT * FROM target_table')
        assert df.count() > 0
How to run the tests:

Save the conftest.py and test_etl.py files in the same directory.
Run the tests using pytest command.
The HTML report will be generated in the same directory as report.html.
How the custom HTML report works:

The conftest.py file defines two hooks: pytest_runtest_makereport and pytest_terminal_summary.
The pytest_runtest_makereport hook captures the test results and stores them in a results list.
The pytest_terminal_summary hook renders the HTML template using Jinja2 and writes the report to a file named report.html.
The HTML template is defined in the REPORT_TEMPLATE variable and uses a simple table format to display the test results.
The test_etl.py file contains the actual test cases,


I am a ETL tester. Create a test suite with pytest.I cant use pytest-html plugin. Create a custom html report in conftest.py.Also collect evidences when we run tests. In which we will have test class name, test case name, outcome pass or fail and evidence in table format. My Usual tests include checking graph file exists, run graph file, check data in source and target hive table using pyspark.



Here's an example of how you can create a test suite with Pytest and a custom HTML report in conftest.py that collects evidence:

conftest.py


import pytest
import os
from datetime import datetime
from jinja2 import Environment, FileSystemLoader

evidences = []

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    outcome = yield
    rep = outcome.get_result()

    if rep.when == "call":
        test_class_name = item.cls.__name__
        test_case_name = item.name
        outcome_text = "Passed" if rep.passed else "Failed"
        evidence = ""

        if not rep.passed:
            if "tmpdir" in item.fixturenames:
                evidence = item.funcargs["tmpdir"]
            else:
                evidence = rep.longreprtext

        evidences.append({
            "test_class_name": test_class_name,
            "test_case_name": test_case_name,
            "outcome": outcome_text,
            "evidence": evidence
        })

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_terminal_summary(terminalreporter, exitstatus, config):
    if exitstatus == 0:
        return

    # Create a Jinja2 environment
    env = Environment(loader=FileSystemLoader(''))

    # Render the HTML template
    template = env.get_template('report_template.html')
    html = template.render(evidences=evidences)

    # Write the HTML report to a file
    with open('report.html', 'w') as f:
        f.write(html)

# Define the HTML template
REPORT_TEMPLATE = '''
<!DOCTYPE html>
<html>
  <head>
    <title>ETL Test Report</title>
    <style>
      table {
        border-collapse: collapse;
      }
      th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;
      }
    </style>
  </head>
  <body>
    <h1>ETL Test Report</h1>
    <table>
      <tr>
        <th>Test Class</th>
        <th>Test Case</th>
        <th>Outcome</th>
        <th>Evidence</th>
      </tr>
      {% for evidence in evidences %}
      <tr>
        <td>{{ evidence.test_class_name }}</td>
        <td>{{ evidence.test_case_name }}</td>
        <td>{{ evidence.outcome }}</td>
        <td>{{ evidence.evidence }}</td>
      </tr>
      {% endfor %}
    </table>
  </body>
</html>
'''

# Create a report_template.html file
with open('report_template.html', 'w') as f:
    f.write(REPORT_TEMPLATE)
test_etl.py


import pytest
from pyspark.sql import SparkSession

@pytest.fixture
def spark():
    return SparkSession.builder.appName("ETL Test").getOrCreate()

class TestETL:
    def test_graph_file_exists(self, spark, tmpdir):
        # Check if graph file exists
        graph_file = 'path/to/graph/file'
        assert os.path.exists(graph_file)
        # Collect evidence
        with open(os.path.join(tmpdir, 'graph_file_exists.txt'), 'w') as f:
            f.write(f"Graph file exists: {graph_file}")

    def test_run_graph_file(self, spark, tmpdir):
        # Run the graph file
        spark.sql('RUN GRAPH path/to/graph/file')
        # Collect evidence
        with open(os.path.join(tmpdir, 'graph_file_run.txt'), 'w') as f:
            f.write("Graph file run successfully")

    def test_source_data(self, spark, tmpdir):
        # Check data in source hive table
        df = spark.sql('SELECT * FROM source_table')
        assert df.count() > 0
        # Collect evidence
        with open(os.path.join(tmpdir, 'source_data.txt'), 'w') as f:
            f.write(f"Source data count: {df.count()}")

    def test_target_data(self, spark, tmpdir):
        # Check data in target hive table
        df = spark.sql('SELECT * FROM target_table')
        assert df.count() > 0
        # Collect evidence
        with open(os.path.join(tmpdir, 'target_data.txt'), 'w') as f:
            f.write(f"Target data count: {df.count()}")
How to run the tests:

Save the conftest.py and test_etl.py files in the same directory.
Run the tests using pytest command.
The HTML report will be generated in the same directory as report.html.
How the custom HTML report works:

The `conftest


I am a ETL tester. Create a test suite with pytest.I cant use pytest-html plugin. Create a custom html report in conftest.py. In which we will have test class name, test case name, outcome pass or fail and evidence in table format. My Usual tests include checking graph file exists, run graph file, check data in source and target hive table using pyspark. Evidences ahould get collected in variable in test case and these evidences should be displayed in html report also.



Here's an example of how you can create a test suite with Pytest and a custom HTML report in conftest.py that collects evidence in a variable and displays it in the HTML report:

conftest.py


import pytest
import os
from datetime import datetime
from jinja2 import Environment, FileSystemLoader

evidences = []

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    outcome = yield
    rep = outcome.get_result()

    if rep.when == "call":
        test_class_name = item.cls.__name__
        test_case_name = item.name
        outcome_text = "Passed" if rep.passed else "Failed"
        evidence = item.funcargs.get('evidence', '')

        evidences.append({
            "test_class_name": test_class_name,
            "test_case_name": test_case_name,
            "outcome": outcome_text,
            "evidence": evidence
        })

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_terminal_summary(terminalreporter, exitstatus, config):
    if exitstatus == 0:
        return

    # Create a Jinja2 environment
    env = Environment(loader=FileSystemLoader(''))

    # Render the HTML template
    template = env.get_template('report_template.html')
    html = template.render(evidences=evidences)

    # Write the HTML report to a file
    with open('report.html', 'w') as f:
        f.write(html)

# Define the HTML template
REPORT_TEMPLATE = '''
<!DOCTYPE html>
<html>
  <head>
    <title>ETL Test Report</title>
    <style>
      table {
        border-collapse: collapse;
      }
      th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;
      }
    </style>
  </head>
  <body>
    <h1>ETL Test Report</h1>
    <table>
      <tr>
        <th>Test Class</th>
        <th>Test Case</th>
        <th>Outcome</th>
        <th>Evidence</th>
      </tr>
      {% for evidence in evidences %}
      <tr>
        <td>{{ evidence.test_class_name }}</td>
        <td>{{ evidence.test_case_name }}</td>
        <td>{{ evidence.outcome }}</td>
        <td>{{ evidence.evidence }}</td>
      </tr>
      {% endfor %}
    </table>
  </body>
</html>
'''

# Create a report_template.html file
with open('report_template.html', 'w') as f:
    f.write(REPORT_TEMPLATE)
test_etl.py


import pytest
from pyspark.sql import SparkSession

@pytest.fixture
def spark():
    return SparkSession.builder.appName("ETL Test").getOrCreate()

class TestETL:
    def test_graph_file_exists(self, spark):
        evidence = ""
        graph_file = 'path/to/graph/file'
        if os.path.exists(graph_file):
            evidence = f"Graph file exists: {graph_file}"
        else:
            evidence = f"Graph file does not exist: {graph_file}"
        assert os.path.exists(graph_file), evidence
        pytest.request.node.funcargs['evidence'] = evidence

    def test_run_graph_file(self, spark):
        evidence = ""
        try:
            spark.sql('RUN GRAPH path/to/graph/file')
            evidence = "Graph file run successfully"
        except Exception as e:
            evidence = f"Error running graph file: {str(e)}"
        assert evidence == "Graph file run successfully", evidence
        pytest.request.node.funcargs['evidence'] = evidence

    def test_source_data(self, spark):
        evidence = ""
        df = spark.sql('SELECT * FROM source_table')
        if df.count() > 0:
            evidence = f"Source data count: {df.count()}"
        else:
            evidence = "No data found in source table"
        assert df.count() > 0, evidence
        pytest.request.node.funcargs['evidence'] = evidence

    def test_target_data(self, spark):
        evidence = ""
        df = spark.sql('SELECT * FROM target_table')
        if df.count() > 0:
            evidence = f"Target data count: {df.count()}"
        else:
            evidence = "No data found in target table"
        assert df.count() > 0, evidence
        pytest.request.node.funcargs['evidence'] = evidence
How to run the tests:

Save the conftest.py and test_etl.py files in the same directory.
Run the tests using pytest command.
The HTML report will be generated in the same directory as report.html.
**How the


Model B

I am a ETL tester. Create a test suite with pytest.I cant use pytest-html plugin. Create a custom html report in conftest.py. In which we will have test class name, test case name, outcome pass or fail and evidence in table format. My Usual tests include checking graph file exists, run graph file, check data in source and target hive table using pyspark.



Creating a custom HTML report for a pytest test suite without using the pytest-html plugin requires modifying the conftest.py file to handle test session starts, test case collection, and the preparation of the final report. I will guide you through setting up a simple test environment, including writing basic tests and configuring conftest.py to generate an HTML report.

Step 1: Setup Your Environment
First, ensure you have Python and pytest installed, along with pyspark if you need to interact with Hive tables.


pip install pytest pyspark
Step 2: Organize Your Project
Organize your project with the following structure:


etl_testing/
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_data_checks.py
‚îÇ
‚îî‚îÄ‚îÄ conftest.py
Step 3: Writing Basic Tests
Here's how you might write some basic tests in test_data_checks.py:


import os
import pytest
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("ETL Test Suite") \
    .enableHiveSupport() \
    .getOrCreate()

class TestGraphFiles:
    def test_graph_file_exists(self):
        assert os.path.exists("path/to/graph/file"), "Graph file does not exist."

    def test_run_graph_file(self):
        result = os.system("run graph file command")
        assert result == 0, "Graph file did not run successfully."

class TestDataIntegrity:
    def test_source_table(self):
        df = spark.sql("SELECT * FROM source_table")
        assert df.count() > 0, "Source table is empty."

    def test_target_table(self):
        df = spark.sql("SELECT * FROM target_table")
        assert df.count() > 0, "Target table is empty."
Step 4: Configuring 
conftest.py
Now, configure conftest.py to capture test results and generate an HTML report:


import pytest
from datetime import datetime
import html

# Initialize a list to collect test results
test_results = []

def pytest_configure(config):
    config._metadata = None

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    # Execute all other hooks to obtain the report object
    outcome = yield
    report = outcome.get_result()
    
    if report.when == 'call':
        test_result = {
            "class": str(item.cls.__name__),
            "name": str(item.originalname),
            "outcome": report.outcome,
            "evidence": "Path to evidence file or details"
        }
        test_results.append(test_result)

def pytest_sessionfinish(session, exitstatus):
    report_file = open('pytest_report.html', 'w')
    html_report = f"<html><head><title>ETL Test Report</title></head><body>"
    html_report += f"<h1>ETL Test Report - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</h1>"
    html_report += "<table border='1'><tr><th>Test Class</th><th>Test Case</th><th>Outcome</th><th>Evidence</th></tr>"
    
    for result in test_results:
        html_report += f"<tr><td>{html.escape(result['class'])}</td>" \
                       f"<td>{html.escape(result['name'])}</td>" \
                       f"<td>{html.escape(result['outcome'])}</td>" \
                       f"<td>{html.escape(result['evidence'])}</td></tr>"
    
    html_report += "</table></body></html>"
    report_file.write(html_report)
    report_file.close()
Explanation
Test Setup: Tests are defined in test_data_checks.py with two classes checking various file and data integrity conditions.

Configuration in conftest.py: This file is set up to:

Collect results from each test case execution.
Append test class name, method name, outcome, and a placeholder for evidence to the test_results list.
Generate an HTML report at the end of the session, listing all test results in a table format.
Step 5: Running Tests
Run your tests using the command:


pytest etl_testing/tests
After executing, you should find an pytest_report.html in your project directory with a detailed report of the test outcomes.

Additional Notes
Evidence Handling: The evidence field is a placeholder. You might want to automate screenshot capture or log retrieval, storing these files in a known location and linking them in this report.
Enhancing Reports: You can enhance report generation with CSS for styling or JavaScript for interactive elements, depending on your needs.
**Security


I am a ETL tester. Create a test suite with pytest.I cant use pytest-html plugin. Create a custom html report in conftest.py.Also collect evidences when we run tests. In which we will have test class name, test case name, outcome pass or fail and evidence in table format. My Usual tests include checking graph file exists, run graph file, check data in source and target hive table using pyspark.



Certainly! To create a custom HTML report in conftest.py for a pytest test suite without using the pytest-html plugin, and to collect evidences during the tests, we'll need to setup your tests to capture and log the necessary information. This will include paths to evidences such as log files or output data snapshots, particularly useful in ETL testing.

Step-by-Step Implementation
Step 1: Environment Setup
First, ensure Python, pytest, and pyspark are installed along with any other necessary packages:


pip install pytest pyspark
Step 2: Project Structure
Set up your project directory as follows:


etl_testing/
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_data_checks.py
‚îÇ
‚îú‚îÄ‚îÄ evidences/
‚îÇ
‚îî‚îÄ‚îÄ conftest.py
Step 3: Writing Tests
In test_data_checks.py, implement tests for checking graph files and data validation in Hive tables:


import os
import subprocess
from pyspark.sql import SparkSession

# Initialize Spark ses


‚öîÔ∏è Arena (battle)
‚öîÔ∏è Arena (side-by-side)
üí¨ Direct Chat
üëÄ Vision Direct Chat
üèÜ Leaderboard
‚ÑπÔ∏è About Us
‚öîÔ∏è  LMSYS Chatbot Arena: Benchmarking LLMs in the Wild
| Blog | GitHub | Paper | Dataset | Twitter | Discord |

üìú Rules
Ask any question to two anonymous models (e.g., ChatGPT, Claude, Llama) and vote for the better one!
You can continue chatting until you identify a winner.
Vote won't be counted if model identity is revealed during conversation.
üèÜ LMSYS Arena 
Leaderboard
We've collected 700,000+ human votes to compute an LLM Elo leaderboard. Find out who is the ü•áLLM Champion!

üëá Chat now!

üîç Expand to see the descriptions of 41 models
‚ñº
Model A

I am a ETL tester. Create a test suite with pytest.I cant use pytest-html plugin. Create a custom html report in conftest.py. In which we will have test class name, test case name, outcome pass or fail and evidence in table format. My Usual tests include checking graph file exists, run graph file, check data in source and target hive table using pyspark.



Here's an example of how you can create a test suite with Pytest and a custom HTML report in conftest.py:

conftest.py


import pytest
from datetime import datetime
from jinja2 import Environment, FileSystemLoader

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    # execute all other hooks to obtain the report object
    outcome = yield
    rep = outcome.get_result()

    # we only look at actual failing test calls, not setup/teardown
    if rep.when == "call" and rep.failed:
        mode = "a" if os.path.exists("failures") else "w"
        with open("failures", mode) as f:
            # let's also access the test function object
            if "tmpdir" in item.fixturenames:
                extra = " (%s)" % item.funcargs["tmpdir"]
            else:
                extra = ""
            f.write(rep.nodeid + extra + "\n")

# Create a custom HTML report
@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_terminal_summary(terminalreporter, exitstatus, config):
    if exitstatus == 0:
        return

    # Get the test results
    results = terminalreporter.stats.get('passed', []) + terminalreporter.stats.get('failed', [])

    # Create a Jinja2 environment
    env = Environment(loader=FileSystemLoader(''))

    # Render the HTML template
    template = env.get_template('report_template.html')
    html = template.render(results=results)

    # Write the HTML report to a file
    with open('report.html', 'w') as f:
        f.write(html)

# Define the HTML template
REPORT_TEMPLATE = '''
<!DOCTYPE html>
<html>
  <head>
    <title>ETL Test Report</title>
    <style>
      table {
        border-collapse: collapse;
      }
      th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;
      }
    </style>
  </head>
  <body>
    <h1>ETL Test Report</h1>
    <table>
      <tr>
        <th>Test Class</th>
        <th>Test Case</th>
        <th>Outcome</th>
        <th>Evidence</th>
      </tr>
      {% for result in results %}
      <tr>
        <td>{{ result.nodeid.split('::')[-2] }}</td>
        <td>{{ result.nodeid.split('::')[-1] }}</td>
        <td>{{ 'Passed' if result.passed else 'Failed' }}</td>
        <td>{{ result.longreprtext if not result.passed else '' }}</td>
      </tr>
      {% endfor %}
    </table>
  </body>
</html>
'''

# Create a report_template.html file
with open('report_template.html', 'w') as f:
    f.write(REPORT_TEMPLATE)
test_etl.py


import pytest
from pyspark.sql import SparkSession

@pytest.fixture
def spark():
    return SparkSession.builder.appName("ETL Test").getOrCreate()

class TestETL:
    def test_graph_file_exists(self, spark):
        # Check if graph file exists
        assert os.path.exists('path/to/graph/file')

    def test_run_graph_file(self, spark):
        # Run the graph file
        spark.sql('RUN GRAPH path/to/graph/file')

    def test_source_data(self, spark):
        # Check data in source hive table
        df = spark.sql('SELECT * FROM source_table')
        assert df.count() > 0

    def test_target_data(self, spark):
        # Check data in target hive table
        df = spark.sql('SELECT * FROM target_table')
        assert df.count() > 0
How to run the tests:

Save the conftest.py and test_etl.py files in the same directory.
Run the tests using pytest command.
The HTML report will be generated in the same directory as report.html.
How the custom HTML report works:

The conftest.py file defines two hooks: pytest_runtest_makereport and pytest_terminal_summary.
The pytest_runtest_makereport hook captures the test results and stores them in a results list.
The pytest_terminal_summary hook renders the HTML template using Jinja2 and writes the report to a file named report.html.
The HTML template is defined in the REPORT_TEMPLATE variable and uses a simple table format to display the test results.
The test_etl.py file contains the actual test cases,


I am a ETL tester. Create a test suite with pytest.I cant use pytest-html plugin. Create a custom html report in conftest.py.Also collect evidences when we run tests. In which we will have test class name, test case name, outcome pass or fail and evidence in table format. My Usual tests include checking graph file exists, run graph file, check data in source and target hive table using pyspark.



Here's an example of how you can create a test suite with Pytest and a custom HTML report in conftest.py that collects evidence:

conftest.py


import pytest
import os
from datetime import datetime
from jinja2 import Environment, FileSystemLoader

evidences = []

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    outcome = yield
    rep = outcome.get_result()

    if rep.when == "call":
        test_class_name = item.cls.__name__
        test_case_name = item.name
        outcome_text = "Passed" if rep.passed else "Failed"
        evidence = ""

        if not rep.passed:
            if "tmpdir" in item.fixturenames:
                evidence = item.funcargs["tmpdir"]
            else:
                evidence = rep.longreprtext

        evidences.append({
            "test_class_name": test_class_name,
            "test_case_name": test_case_name,
            "outcome": outcome_text,
            "evidence": evidence
        })

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_terminal_summary(terminalreporter, exitstatus, config):
    if exitstatus == 0:
        return

    # Create a Jinja2 environment
    env = Environment(loader=FileSystemLoader(''))

    # Render the HTML template
    template = env.get_template('report_template.html')
    html = template.render(evidences=evidences)

    # Write the HTML report to a file
    with open('report.html', 'w') as f:
        f.write(html)

# Define the HTML template
REPORT_TEMPLATE = '''
<!DOCTYPE html>
<html>
  <head>
    <title>ETL Test Report</title>
    <style>
      table {
        border-collapse: collapse;
      }
      th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;
      }
    </style>
  </head>
  <body>
    <h1>ETL Test Report</h1>
    <table>
      <tr>
        <th>Test Class</th>
        <th>Test Case</th>
        <th>Outcome</th>
        <th>Evidence</th>
      </tr>
      {% for evidence in evidences %}
      <tr>
        <td>{{ evidence.test_class_name }}</td>
        <td>{{ evidence.test_case_name }}</td>
        <td>{{ evidence.outcome }}</td>
        <td>{{ evidence.evidence }}</td>
      </tr>
      {% endfor %}
    </table>
  </body>
</html>
'''

# Create a report_template.html file
with open('report_template.html', 'w') as f:
    f.write(REPORT_TEMPLATE)
test_etl.py


import pytest
from pyspark.sql import SparkSession

@pytest.fixture
def spark():
    return SparkSession.builder.appName("ETL Test").getOrCreate()

class TestETL:
    def test_graph_file_exists(self, spark, tmpdir):
        # Check if graph file exists
        graph_file = 'path/to/graph/file'
        assert os.path.exists(graph_file)
        # Collect evidence
        with open(os.path.join(tmpdir, 'graph_file_exists.txt'), 'w') as f:
            f.write(f"Graph file exists: {graph_file}")

    def test_run_graph_file(self, spark, tmpdir):
        # Run the graph file
        spark.sql('RUN GRAPH path/to/graph/file')
        # Collect evidence
        with open(os.path.join(tmpdir, 'graph_file_run.txt'), 'w') as f:
            f.write("Graph file run successfully")

    def test_source_data(self, spark, tmpdir):
        # Check data in source hive table
        df = spark.sql('SELECT * FROM source_table')
        assert df.count() > 0
        # Collect evidence
        with open(os.path.join(tmpdir, 'source_data.txt'), 'w') as f:
            f.write(f"Source data count: {df.count()}")

    def test_target_data(self, spark, tmpdir):
        # Check data in target hive table
        df = spark.sql('SELECT * FROM target_table')
        assert df.count() > 0
        # Collect evidence
        with open(os.path.join(tmpdir, 'target_data.txt'), 'w') as f:
            f.write(f"Target data count: {df.count()}")
How to run the tests:

Save the conftest.py and test_etl.py files in the same directory.
Run the tests using pytest command.
The HTML report will be generated in the same directory as report.html.
How the custom HTML report works:

The `conftest

Model B

I am a ETL tester. Create a test suite with pytest.I cant use pytest-html plugin. Create a custom html report in conftest.py. In which we will have test class name, test case name, outcome pass or fail and evidence in table format. My Usual tests include checking graph file exists, run graph file, check data in source and target hive table using pyspark.



Creating a custom HTML report for a pytest test suite without using the pytest-html plugin requires modifying the conftest.py file to handle test session starts, test case collection, and the preparation of the final report. I will guide you through setting up a simple test environment, including writing basic tests and configuring conftest.py to generate an HTML report.

Step 1: Setup Your Environment
First, ensure you have Python and pytest installed, along with pyspark if you need to interact with Hive tables.


pip install pytest pyspark
Step 2: Organize Your Project
Organize your project with the following structure:


etl_testing/
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_data_checks.py
‚îÇ
‚îî‚îÄ‚îÄ conftest.py
Step 3: Writing Basic Tests
Here's how you might write some basic tests in test_data_checks.py:


import os
import pytest
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("ETL Test Suite") \
    .enableHiveSupport() \
    .getOrCreate()

class TestGraphFiles:
    def test_graph_file_exists(self):
        assert os.path.exists("path/to/graph/file"), "Graph file does not exist."

    def test_run_graph_file(self):
        result = os.system("run graph file command")
        assert result == 0, "Graph file did not run successfully."

class TestDataIntegrity:
    def test_source_table(self):
        df = spark.sql("SELECT * FROM source_table")
        assert df.count() > 0, "Source table is empty."

    def test_target_table(self):
        df = spark.sql("SELECT * FROM target_table")
        assert df.count() > 0, "Target table is empty."
Step 4: Configuring 
conftest.py
Now, configure conftest.py to capture test results and generate an HTML report:


import pytest
from datetime import datetime
import html

# Initialize a list to collect test results
test_results = []

def pytest_configure(config):
    config._metadata = None

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    # Execute all other hooks to obtain the report object
    outcome = yield
    report = outcome.get_result()
    
    if report.when == 'call':
        test_result = {
            "class": str(item.cls.__name__),
            "name": str(item.originalname),
            "outcome": report.outcome,
            "evidence": "Path to evidence file or details"
        }
        test_results.append(test_result)

def pytest_sessionfinish(session, exitstatus):
    report_file = open('pytest_report.html', 'w')
    html_report = f"<html><head><title>ETL Test Report</title></head><body>"
    html_report += f"<h1>ETL Test Report - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</h1>"
    html_report += "<table border='1'><tr><th>Test Class</th><th>Test Case</th><th>Outcome</th><th>Evidence</th></tr>"
    
    for result in test_results:
        html_report += f"<tr><td>{html.escape(result['class'])}</td>" \
                       f"<td>{html.escape(result['name'])}</td>" \
                       f"<td>{html.escape(result['outcome'])}</td>" \
                       f"<td>{html.escape(result['evidence'])}</td></tr>"
    
    html_report += "</table></body></html>"
    report_file.write(html_report)
    report_file.close()
Explanation
Test Setup: Tests are defined in test_data_checks.py with two classes checking various file and data integrity conditions.

Configuration in conftest.py: This file is set up to:

Collect results from each test case execution.
Append test class name, method name, outcome, and a placeholder for evidence to the test_results list.
Generate an HTML report at the end of the session, listing all test results in a table format.
Step 5: Running Tests
Run your tests using the command:


pytest etl_testing/tests
After executing, you should find an pytest_report.html in your project directory with a detailed report of the test outcomes.

Additional Notes
Evidence Handling: The evidence field is a placeholder. You might want to automate screenshot capture or log retrieval, storing these files in a known location and linking them in this report.
Enhancing Reports: You can enhance report generation with CSS for styling or JavaScript for interactive elements, depending on your needs.
**Security


I am a ETL tester. Create a test suite with pytest.I cant use pytest-html plugin. Create a custom html report in conftest.py.Also collect evidences when we run tests. In which we will have test class name, test case name, outcome pass or fail and evidence in table format. My Usual tests include checking graph file exists, run graph file, check data in source and target hive table using pyspark.



Certainly! To create a custom HTML report in conftest.py for a pytest test suite without using the pytest-html plugin, and to collect evidences during the tests, we'll need to setup your tests to capture and log the necessary information. This will include paths to evidences such as log files or output data snapshots, particularly useful in ETL testing.

Step-by-Step Implementation
Step 1: Environment Setup
First, ensure Python, pytest, and pyspark are installed along with any other necessary packages:


pip install pytest pyspark
Step 2: Project Structure
Set up your project directory as follows:


etl_testing/
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_data_checks.py
‚îÇ
‚îú‚îÄ‚îÄ evidences/
‚îÇ
‚îî‚îÄ‚îÄ conftest.py
Step 3: Writing Tests
In test_data_checks.py, implement tests for checking graph files and data validation in Hive tables:


import os
import subprocess
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("ETL Test Suite") \
    .enableHiveSupport() \
    .getOrCreate()

class TestGraphFiles:
    def test_graph_file_exists(self):
        assert os.path.exists("path/to/graph/file"), "Graph file does not exist."

    def test_run_graph_file(self):
        process = subprocess.run(["command", "to", "run", "graph"], capture_output=True, text=True)
        assert process.returncode == 0, f"Graph file did not run successfully. Error: {process.stderr}"
        # Save output to a file
        with open('evidences/graph_run_output.txt', 'w') as f:
            f.write(process.stdout)
        self.evidence = 'evidences/graph_run_output.txt'

class TestDataIntegrity:
    def test_source_table(self):
        df = spark.sql("SELECT * FROM source_table")
        df.persist()
        assert df.count() > 0, "Source table is empty."
        df.sample(False, 0.1).toPandas().to_csv('evidences/source_table_sample.csv')
        self.evidence = 'evidences/source_table_sample.csv'

    def test_target_table(self):
        df = spark.sql("SELECT * FROM target_table")
        df.persist()
        assert df.count() > 0, "Target table is empty."
        df.sample(False, 0.1).toPandas().to_csv('evidences/target_table_sample.csv')
        self.evidence = 'evidences/target_table_sample.csv'
Step 4: Configuring 
conftest.py
Modify conftest.py to generate an HTML report that includes links to evidences:


import pytest
from datetime import datetime
import html

test_results = []

def pytest_configure(config):
    config._metadata = None

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    outcome = yield
    report = outcome.get_result()
    
    if report.when == 'call':
        test_class = item.cls.__name__ if item.cls else ''
        test_name = item.originalname
        evidence_path = getattr(item.obj, "evidence", "No evidence collected")
        test_result = {
            "class": test_class,
            "name": test_name,
            "outcome": report.outcome,
            "evidence": evidence_path
        }
        test_results.append(test_result)

def pytest_sessionfinish(session, exitstatus):
    report_file = open('pytest_report.html', 'w')
    html_report = f"<html><head><title>ETL Test Report</title></head><body>"
    html_report += f"<h1>ETL Test Report - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</h1>"
    html_report += "<table border='1'><tr><th>Test Class</th><th>Test Case</th><th>Outcome</th><th>Evidence</th></tr>"
    
    for result in test_results:
        html_report += f"<tr><td>{html.escape(result['class'])}</td>" \
                       f"<td>{html.escape(result['name'])}</td>" \
                       f"<td>{html.escape(result['outcome'])}</td>" \
                       f"<td><a href='{html.escape(result['evidence'])}'>{html.escape(result['evidence'])}</a></td></tr>"
    
    html_report += "</table></body></html>"
    report_file.write(html_report)
    report_file.close()
Explanation:
Tests: Each test method checks a condition and logs the output or a sample of the data to the evidences/ directory. It then sets an attribute for the evidence file path which is used

Model A: llama-3-70b-instruct
Model B: gpt2-chatbot
üëà A is better
üëâ B is better
ü§ù Tie
üëé Both are bad
üëâ Enter your prompt and press ENTER
Send
üé≤ New Round
üîÑ Regenerate
üì∑ Share
Terms of Service
Users are required to agree to the following terms before using the service:

The service is a research preview. It only provides limited safety measures and may generate offensive content. It must not be used for any illegal, harmful, violent, racist, or sexual purposes. Please do not upload any private information. The service collects user dialogue data, including both text and images, and reserves the right to distribute it under a Creative Commons Attribution (CC-BY) or a similar license.

Acknowledgment
We thank UC Berkeley SkyLab, Kaggle, MBZUAI, a16z, Together AI, Hyperbolic, Anyscale, HuggingFace for their generous sponsorship.

SkyLabKaggleMBZUAIa16zTogether AIHyperbolicAnyScaleHuggingFace
Built with Gradiologo


Sure, here's an example to illustrate `${2-1}` in a shell script:

```bash
#!/bin/bash

# Check if the second positional parameter is set, otherwise default to 1
second_param=${2-1}

echo "Second parameter is: $second_param"
```

Now, let's say you run this script with different arguments:

1. If you run `./script.sh arg1`, the output will be `Second parameter is: 1` because the second positional parameter is not set, so it defaults to `1`.
2. If you run `./script.sh arg1 arg2`, the output will be `Second parameter is: arg2` because the second positional parameter is set to `arg2`.

In summary, `${2-1}` checks if the second positional parameter is set, and if it's no


# conftest.py

def pytest_addoption(parser):
    parser.addoption("--email", action="store", default=None, help="Email address for sending reports")

def pytest_configure(config):
    email = config.getoption("--email")
    if email:
        config.email_address = email

from pyspark.sql import SparkSession
import pandas as pd

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Save to Excel") \
    .getOrCreate()

# Sample SQL query and PySpark DataFrame creation
sql_query = """
SELECT Name, Age
FROM (
    VALUES 
        ('Alice', 1),
        ('Bob', 2),
        ('Charlie', 3)
) AS tempTable(Name, Age)
"""

# Count the number of lines in the SQL query
sql_lines_count = len(sql_query.strip().split('\n'))

# Add 2 to the SQL lines count to determine the starting row for DataFrame in Excel
start_row_df = sql_lines_count + 2

# Print the SQL lines count
print(f"Number of lines in SQL code: {sql_lines_count}")

# Execute the SQL query and create a PySpark DataFrame
df = spark.sql(sql_query)

# Convert PySpark DataFrame to Pandas DataFrame
pandas_df = df.toPandas()

# Define the path to save the Excel file
excel_file_path = "path_to_save/sql_and_dataframe.xlsx"

# Create Excel writer object
writer = pd.ExcelWriter(excel_file_path, engine='xlsxwriter')

# Add SQL query as header in the same sheet
sql_query_df = pd.DataFrame({"SQL Query": [sql_query]})
sql_query_df.to_excel(writer, sheet_name='Sheet1', startrow=0, index=False, header=False)

# Write the Pandas DataFrame to the same sheet, starting from the calculated start row
pandas_df.to_excel(writer, sheet_name='Sheet1', startrow=start_row_df, index=False)

# Save the Excel file
writer.save()

# Stop Spark session
spark.stop()




#!/bin/bash

# Check if correct number of arguments are provided
if [ "$#" -ne 2 ]; then
    echo "Usage: $0 <part_of_link> <mail_id>"
    exit 1
fi

# Assign arguments to variables
part_of_link="$1"
mail_id="$2"

# Define log file path
log_file="/home/script_log_$(date +'%Y-%m-%d_%H-%M-%S').txt"

# Define HTML report file path
html_report="/home/diff_report_$(date +'%Y-%m-%d_%H-%M-%S').html"

# Function to log commands and their output
log_command() {
    local command="$1"
    echo "### Running command: $command" | tee -a "$log_file"
    eval "$command" 2>&1 | tee -a "$log_file"
    echo "### End of command output" | tee -a "$log_file"
}

# Log start time
echo "Script started at: $(date +'%Y-%m-%d %H:%M:%S')" | tee "$log_file"

# Check if any files or folders exist in /home/sit_path
if [ "$(ls -A /home/sit_path)" ]; then
    log_command "rm -r /home/sit_path"
else
    echo "No files or folders found in /home/sit_path. Skipping removal." | tee -a "$log_file"
fi

# Download the tar file using curl to a specific directory
download_directory="/path/to/directory"  # Change this to your desired directory
log_command "curl -u username:password -o $download_directory/$part_of_link <link>/$part_of_link"

# Check if the downloaded file exists
if [ ! -f "$download_directory/$part_of_link" ]; then
    echo "Error: Downloaded file not found." | tee -a "$log_file"
    exit 1
fi

# Extract the contents of the tar file
log_command "tar -xf $download_directory/$part_of_link -C $download_directory"

# Compare contents of Track1 folders in SIT and PROD
diff_output=$(diff -rq /home/SIT/Track1 /home/PROD/Track1)

# Log the diff output
echo "### Directory Comparison Output:" | tee -a "$log_file"
echo "$diff_output" | tee -a "$log_file"

# Create HTML report
echo "<html>
<head><title>Folder Comparison Report</title></head>
<body>
<h1>Folder Comparison Report</h1>
<h2>Directory Comparison Output:</h2>
<pre>$diff_output</pre>
<h2>File Differences:</h2>
<ul>" > "$html_report"

# Check if there are differences
if [ -n "$diff_output" ]; then
    # If differences exist, find and log differences in specific files
    while IFS= read -r line; do
        file=$(echo "$line" | awk '{print $2}')
        log_command "diff /home/SIT/Track1/$file /home/PROD/Track1/$file"
        echo "<li><b>File: $file</b><br><pre>" >> "$html_report"
        diff /home/SIT/Track1/$file /home/PROD/Track1/$file >> "$html_report" 2>&1
        echo "</pre></li>" >> "$html_report"
    done < <(echo "$diff_output" | awk '{print $2}')
fi

# Complete the HTML report
echo "</ul>
</body>
</html>" >> "$html_report"

# Log end time
echo "Script ended at: $(date +'%Y-%m-%d %H:%M:%S')" | tee -a "$log_file"

# Send the log file and HTML report via email
mailx -s "Script Execution Log" -a "$html_report" $mail_id < "$log_file"

# Clean up temporary files
rm "$log_file" "$html_report"

SELECT *
FROM (
    SELECT *
    FROM (
        SELECT *
        FROM (
            SELECT *
            FROM tableA AS a
            INNER JOIN tableB AS b ON a.cus_id = b.cus_id AND b.active = 1
        ) AS flow_a
        INNER JOIN tableC AS c ON flow_a.cus_id = c.cus_id AND c.active = 1
        INNER JOIN tableD AS d ON flow_a.cus_id = d.cus_id AND d.active = 1
    ) AS flow_b
    LEFT OUTER JOIN tableE AS e ON flow_b.sortid = e.sortid AND flow_b.cus_acc_id = e.cus_acc_id AND e.active = 1
) AS flow_c
LEFT OUTER JOIN tableF AS f ON flow_c.stg_id = f.stg_id AND f.active = 1
LEFT OUTER JOIN tableG AS g ON flow_c.stg_id = g.stg_id AND g.active = 1
LEFT OUTER JOIN tableH AS h ON flow_c.stg_id = h.stg_id AND h.active = 1
WHERE flow_c.active = 1;



SELECT *
FROM (
    SELECT *
    FROM (
        SELECT *
        FROM (
            SELECT *
            FROM tableA AS a
            INNER JOIN tableB AS b ON a.cus_id = b.cus_id
        ) AS flow_a
        INNER JOIN tableC AS c ON flow_a.cus_id = c.cus_id
        INNER JOIN tableD AS d ON flow_a.cus_id = d.cus_id
    ) AS flow_b
    LEFT OUTER JOIN tableE AS e ON flow_b.sortid = e.sortid AND flow_b.cus_acc_id = e.cus_acc_id
) AS flow_c
LEFT OUTER JOIN tableF AS f ON flow_c.stg_id = f.stg_id
LEFT OUTER JOIN tableG AS g ON flow_c.stg_id = g.stg_id
LEFT OUTER JOIN tableH AS h ON flow_c.stg_id = h.stg_id;




#!/bin/bash

# Check if correct number of arguments are provided
if [ "$#" -ne 2 ]; then
    echo "Usage: $0 <part_of_link> <mail_id>"
    exit 1
fi

# Assign arguments to variables
part_of_link="$1"
mail_id="$2"

# Remove the existing directory
rm -r /home/sit_path

# Download the tar file using curl
curl -O $USER <link>/$part_of_link

# Extract the contents of the tar file
tar -xf <downloaded_file.tar>

# Compare contents of Track1 folders in SIT and PROD
diff_output=$(diff -rq /home/SIT/Track1 /home/PROD/Track1)

# Extract specific lines from adbs files
sed_sit=$(sed -n '7p' /home/SIT/Track1/adbs)
sed_prod=$(sed -n '7p' /home/PROD/Track1/adbs)

# Grep 'Inputfile' from wrapper.ksh files
grep_sit=$(grep 'Inputfile' /home/SIT/Track1/wrapper.ksh)
grep_prod=$(grep 'Inputfile' /home/PROD/Track1/wrapper.ksh)

# Create HTML report
html_report="/home/diff_report.html"
echo "<html>
<head><title>Folder Comparison Report</title></head>
<body>
<h1>Folder Comparison Report</h1>
<h2>Folder Difference:</h2>
<pre>$diff_output</pre>
<h2>SIT adbs 7th Line:</h2>
<pre>$sed_sit</pre>
<h2>PROD adbs 7th Line:</h2>
<pre>$sed_prod</pre>
<h2>SIT wrapper.ksh Inputfile:</h2>
<pre>$grep_sit</pre>
<h2>PROD wrapper.ksh Inputfile:</h2>
<pre>$grep_prod</pre>
</body>
</html>" > "$html_report"

# Send the HTML report via email
mailx -s "Folder Comparison Report" $mail_id < "$html_report"

# Clean up temporary files
rm "$html_report"

#!/bin/bash

# Check if correct number of arguments are provided
if [ "$#" -ne 2 ]; then
    echo "Usage: $0 <part_of_link> <mail_id>"
    exit 1
fi

# Assign arguments to variables
part_of_link="$1"
mail_id="$2"

# Define log file path
log_file="/home/script_log_$(date +'%Y-%m-%d_%H-%M-%S').txt"

# Function to log commands and their output
log_command() {
    local command="$1"
    echo "### Running command: $command" >> "$log_file"
    eval "$command" >> "$log_file" 2>&1
    echo "### End of command output" >> "$log_file"
}

# Log start time
echo "Script started at: $(date +'%Y-%m-%d %H:%M:%S')" > "$log_file"

# Remove the existing directory
log_command "rm -r /home/sit_path"

# Download the tar file using curl
log_command "curl -u username:password -O $USER <link>/$part_of_link"

# Check if the downloaded file exists
if [ ! -f "$part_of_link" ]; then
    echo "Error: Downloaded file not found." >> "$log_file"
    exit 1
fi

# Extract the contents of the tar file
log_command "tar -xf $part_of_link"

# Continue with the rest of the script...

# Log end time
echo "Script ended at: $(date +'%Y-%m-%d %H:%M:%S')" >> "$log_file"

# Send the log file via email
mailx -s "Script Execution Log" $mail_id < "$log_file"

# Clean up temporary log file
rm "$log_file"


import json

def read_json_file(file_path, field, start_record, end_record):
    try:
        # Open the JSON file
        with open(file_path, 'r') as file:
            data = json.load(file)

        # Traverse through nested structure to get the desired field value
        def get_nested_value(record, nested_fields):
            nested_value = record
            for nested_field in nested_fields:
                if isinstance(nested_value, dict):
                    nested_value = nested_value.get(nested_field)
                elif isinstance(nested_value, list) and nested_field.isdigit():
                    nested_value = nested_value[int(nested_field)]
                else:
                    nested_value = None
                    break
            return nested_value

        # Check if the specified field is valid
        if field in data[0]:
            # Iterate through the specified range of records and print the field value
            for i in range(start_record, min(end_record + 1, len(data) + 1)):
                record = data[i - 1]  # Adjust index since records start from 1
                
                # Split nested field name by dot to handle nested fields
                nested_fields = field.split('.')
                
                # Get the value from nested structure
                nested_value = get_nested_value(record, nested_fields)
                
                if nested_value is not None:
                    print(f"Record {i} - {field}: {nested_value}")
                else:
                    print(f"Nested field '{field}' not found in record {i}.")
        else:
            print(f"Field '{field}' not found in the JSON data.")

    except FileNotFoundError:
        print("File not found. Please enter a valid file path.")
    except IndexError:
        print("Invalid record range. Please enter a valid range.")

def main():
    action = input("Do you want to read or edit the JSON file? (read/edit): ")

    if action.lower() == 'read':
        file_path = input("Enter the path of the JSON file: ")
        field = input("Enter the field to read: ")
        start_record = int(input("Enter the starting record number: "))
        end_record = int(input("Enter the ending record number: "))

        # Call the function to read and print JSON data
        read_json_file(file_path, field, start_record, end_record)
    elif action.lower() == 'edit':
        start_record = int(input("Enter the starting record number: "))
        end_record = int(input("Enter the ending record number: "))
        file_path = input("Enter the path of the JSON file: ")

        edit_fields = {}
        edit_more = True
        while edit_more:
            # Ask user for the field to edit
            edit_field = input("Enter the field to edit: ")

            # Ask user if the value is an integer or a string
            value_type = input("Is the value an integer or a string? (integer/string): ")

            # Ask user for the new value
            new_value = input(f"Enter the new value for {edit_field}: ")

            # Convert the new value based on user input
            if value_type.lower() == 'integer':
                new_value = int(new_value)

            edit_fields[edit_field] = new_value

            edit_more_input = input("Do you want to edit more fields? (yes/no): ")
            edit_more = edit_more_input.lower() == 'yes'

        # Call the function to edit the JSON file with multiple edit fields
        edit_json_file(start_record, end_record, file_path, edit_fields)
    else:
        print("Invalid action. Please choose 'read' or 'edit'.")

# Call the main function to start the script
main()











import json

def edit_json_file(start_record, end_record, file_path, edit_fields):
    try:
        # Open the JSON file
        with open(file_path, 'r') as file:
            data = json.load(file)

        # Iterate through the specified range of records
        for i in range(start_record, end_record + 1):
            record = data[i - 1]  # Adjust index since records start from 1

            # Update the specified fields with the new values
            for field, new_value in edit_fields.items():
                # Split the field name by dot to handle nested fields
                nested_fields = field.split('.')
                nested_record = record
                for nested_field in nested_fields[:-1]:
                    if nested_field.isdigit():  # Check if it's an integer index
                        nested_record = nested_record[int(nested_field)]
                    else:
                        nested_record = nested_record.get(nested_field, {})
                last_field = nested_fields[-1]

                # Update the last nested field with the new value
                if isinstance(nested_record, list):
                    print(f"Field '{field}' is not a dictionary. Skipping update.")
                elif last_field in nested_record:
                    nested_record[last_field] = new_value
                else:
                    print(f"Field '{last_field}' not found in record {i}. Skipping update.")

        # Save the updated data back to the JSON file without double quotes
        with open(file_path, 'w') as file:
            json.dump(data, file, indent=4)

        print("JSON file updated successfully!")

    except FileNotFoundError:
        print("File not found. Please enter a valid file path.")
    except IndexError:
        print("Invalid record range. Please enter a valid range.")

def read_json_file(file_path, field, start_record, end_record):
    try:
        # Open the JSON file
        with open(file_path, 'r') as file:
            data = json.load(file)

        # Check if the specified field is valid
        if field in data[0]:
            # Iterate through the specified range of records and print the field value
            for i in range(start_record, min(end_record + 1, len(data) + 1)):
                record = data[i - 1]  # Adjust index since records start from 1
                value = record.get(field)
                print(f"Record {i} - {field}: {value}")
        else:
            print(f"Field '{field}' not found in the JSON data.")

    except FileNotFoundError:
        print("File not found. Please enter a valid file path.")
    except IndexError:
        print("Invalid record range. Please enter a valid range.")

def main():
    action = input("Do you want to read or edit the JSON file? (read/edit): ")

    if action.lower() == 'read':
        file_path = input("Enter the path of the JSON file: ")
        field = input("Enter the field to read: ")
        start_record = int(input("Enter the starting record number: "))
        end_record = int(input("Enter the ending record number: "))

        # Call the function to read and print JSON data
        read_json_file(file_path, field, start_record, end_record)
    elif action.lower() == 'edit':
        start_record = int(input("Enter the starting record number: "))
        end_record = int(input("Enter the ending record number: "))
        file_path = input("Enter the path of the JSON file: ")

        edit_fields = {}
        edit_more = True
        while edit_more:
            # Ask user for the field to edit
            edit_field = input("Enter the field to edit: ")

            # Ask user if the value is an integer or a string
            value_type = input("Is the value an integer or a string? (integer/string): ")

            # Ask user for the new value
            new_value = input(f"Enter the new value for {edit_field}: ")

            # Convert the new value based on user input
            if value_type.lower() == 'integer':
                new_value = int(new_value)

            edit_fields[edit_field] = new_value

            edit_more_input = input("Do you want to edit more fields? (yes/no): ")
            edit_more = edit_more_input.lower() == 'yes'

        # Call the function to edit the JSON file with multiple edit fields
        edit_json_file(start_record, end_record, file_path, edit_fields)
    else:
        print("Invalid action. Please choose 'read' or 'edit'.")

# Call the main function to start the script
main()



----------++-++++--------------
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a Spark session
spark = SparkSession.builder \
    .appName("Fill Null Values") \
    .getOrCreate()

# Sample DataFrame
data = [(1, None), (2, 3), (None, 4)]
columns = ["a", "b"]
df = spark.createDataFrame(data, columns)

# Update column 'a' with value 0 where it is null
df_filled = df.fillna(0, subset=["a"])

# Show the updated DataFrame
df_filled.show()

To run pytest commands with Spark submit, you would typically not use Spark submit directly for pytest. Instead, you would run pytest as you normally would, and within your tests, you would configure and use SparkContext or SparkSession as needed. Pytest is a testing framework for Python, and it doesn't directly interact with Spark's submission mechanism. However, you can configure your tests to use Spark in a local or distributed mode, depending on your requirements.

Here's a step-by-step guide on how to set up and run pytest for testing PySpark applications:

1. **Install pytest and pytest-spark**: First, ensure you have pytest installed in your environment. You can install it using pip:
   ```bash
   pip install pytest
   ```
   Additionally, if you're working with PySpark, you might find the `pytest-spark` plugin useful. It provides fixtures for SparkContext and SparkSession, making it easier to test PySpark applications. Install it with:
   ```bash
   pip install pytest-spark
   ```

2. **Configure pytest for Spark**: If you're using `pytest-spark`, you can specify the `SPARK_HOME` directory in your `pytest.ini` file or via the command line using the `--spark_home` option. This is necessary for `pytest-spark` to locate Spark and make it available for your tests.

3. **Write your tests**: Use pytest fixtures to create SparkContext or SparkSession instances for your tests. Here's an example of how to set up a fixture for SparkContext:
   ```python
   import pytest
   from pyspark import SparkConf, SparkContext

   @pytest.fixture(scope="session")
   def spark_context(request):
       conf = SparkConf().setMaster("local[3]").setAppName("pytest-pyspark-local-testing")
       sc = SparkContext(conf=conf)
       request.addfinalizer(lambda: sc.stop())
       return sc
   ```
   This fixture can be used in your tests to get a SparkContext instance.

4. **Run your tests**: Execute your tests using the `pytest` command. If you're using markers to differentiate between local and YARN mode tests, you can run them accordingly:
   ```bash
   pytest -m spark_local
   ```
   or
   ```bash
   pytest -m spark_yarn
   ```
   This will run tests marked with `@pytest.mark.spark_local` or `@pytest.mark.spark_yarn` depending on the mode you're targeting.

5. **Mocking external dependencies**: If your PySpark code interacts with external systems (like databases or file systems), consider using mocking libraries like `unittest.mock` or `pytest-mock` to simulate these interactions in your tests. This helps in isolating the code being tested and makes your tests more reliable and faster.

Remember, the key to testing PySpark applications with pytest is to set up your Spark environment within your tests using fixtures, and then write tests that interact with this environment. This approach allows you to test your PySpark code in a controlled environment, without needing to submit your tests to Spark.

Citations:
[1] https://stackoverflow.com/questions/40975360/testing-spark-with-pytest-cannot-run-spark-in-local-mode
[2] https://pypi.org/project/pytest-spark/
[3] https://spark.apache.org/docs/latest/api/python/getting_started/testing_pyspark.html
[4] https://medium.com/@shuklaprashant9264/pytest-with-pyspark-70821cd778a9
[5] https://levelup.gitconnected.com/4-tips-to-integrate-pytest-with-pyspark-applications-5a18835a6d3e
[6] https://mikulskibartosz.name/use-one-spark-session-to-run-all-pytest-tests
[7] https://medium.com/towards-data-engineering/how-to-write-pytest-for-pyspark-application-5d8497633c77
[8] https://docs.gcp.databricks.com/en/dev-tools/vscode-ext/dev-tasks/pytest.html
[9] https://docs.pytest.org/en/7.1.x/how-to/usage.html
[10] https://garybake.com/pyspark_pytest.html



pytest --log-cli-format="%(asctime)s %(levelname)s %(message)s" --log-file-format="%(asctime)s %(levelname)s %(message)s"


import logging

custom_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler = logging.StreamHandler()
handler.setFormatter(custom_formatter)
logging.getLogger().addHandler(handler)



from pyspark.sql import SparkSession, functions as F

spark = SparkSession.builder.getOrCreate()

# Sample DataFrames (replace with your actual data)
df1 = spark.createDataFrame([("A1", 10, True), ("A2", 20, False), ("A3", 30, True)], 
                           ["customerid", "value1", "condition1"])
df2 = spark.createDataFrame([("A1", "X"), ("A2", "Y"), ("B1", "Z")], ["customerid", "col_B"])

# Join DataFrames on customerid
joined_df = df1.join(df2, on="customerid", how="left")

# Define validation conditions (replace with your actual conditions)
validation_1 = joined_df["value1"] > 25
validation_2 = joined_df["condition1"] == True

# Filter rows where all validation conditions are met
valid_df = joined_df.where(validation_1 & validation_2) \
                  .select("customerid", "col_B")  # Select relevant columns

# Check if any rows meet the criteria and have the expected value in col_B
expected_value = "specific_value"  # Replace with your desired value
if valid_df.filter(F.col("col_B") == expected_value).count() > 0:
  print(f"Validation successful: col_B has '{expected_value}' for customers meeting all conditions")
else:
  print("Validation failed: No rows meet all conditions and have the expected value in col_B")



from pyspark.sql import SparkSession, functions as F

spark = SparkSession.builder.getOrCreate()

# Sample DataFrame (replace with your actual data)
data = [("A", 0), ("B", 10), ("C", 5), ("D", 0), ("E", 2)]
df = spark.createDataFrame(data, ["col_A", "col_B"])

# Define the list of valid values for col_A
valid_values = ["A", "B", "C"]

# Filter rows where col_A is not in the valid list
invalid_df = df.where(~F.col("col_A").isin(valid_values))

# Check if there are any invalid rows
if invalid_df.isEmpty():
  print("Validation successful: All rows with A, B, or C in col_A have 0 in col_B")
else:
  print("Validation failed:")
  invalid_df.show()  # Display rows that failed validation


from pyspark.sql import SparkSession, functions as F

spark = SparkSession.builder.getOrCreate()

# Sample DataFrames (replace with your actual data)
df1 = spark.createDataFrame([("A", 10), ("B", 20), ("C", 30)], ["id", "value"])
df2 = spark.createDataFrame([("A", 10), ("B", 25), ("D", 40)], ["id", "value"])

# Row-wise difference
diff_df = df1.subtract(df2)
diff_df.show()  # Show rows present only in df1

# Column-wise comparison
df1.select(F.col("id") == df2.col("id"), F.col("value") == df2.col("value")).show()

# Counting rows with different values in a specific column
mismatch_count = df1.where(F.col("id") != df2.col("id")).count()
print(f"Number of rows with ID mismatch: {mismatch_count}")

if df1.dtypes == df2.dtypes:
    print("DataFrames have matching schemas")
else:
    print("Schema mismatch between DataFrames")


import os
import subprocess

def get_file_listing(file_path):
    assert os.path.exists(file_path), f"{file_path} does not exist"
    ls_output = subprocess.run(["ls", "-lrt", file_path], capture_output=True, text=True)
    return ls_output.stdout

def test_file_listing():
    file_path = "example.txt"  # Change to your file path
    listing = get_file_listing(file_path)
    print(f"Listing of {file_path}:\n{listing}")
------++---------

def append_to_list(*args):
    result_list = []
    for arg in args:
        result_list.append(arg)
    return result_list

import csv

# Open the file
with open('your_file.txt', 'r') as file:
    # Create a CSV reader object with tab delimiter
    reader = csv.reader(file, delimiter='\t')
    
    # Iterate over each row
    for row in reader:
        # Check if the second column is '64'
        if len(row) > 1 and row[1] == '64':
            # If it is, do something with the row
            print(row)





def test_file_exists(file_path):
    assert os.path.exists(file_path)

def test_file_content(file_path):
    with open(file_path, "r") as f:
        content = f.read()
    assert "pytest-html" in content

def test_file_size(file_path):
    size = os.path.getsize(file_path)
    assert size > 0
--------------------------------------------------

import pytest
from your_module import compare_versions

@pytest.mark.parametrize("path, expected_result", [
    ("/path/to/sandbox1", True),  # Example where latest version and sandbox version are the same
    ("/path/to/sandbox2", False), # Example where latest version and sandbox version are different
])
def test_compare_versions(path, expected_result):
    result = compare_versions(path)
    assert result == expected_result, f"Unexpected result for path '{path}'. Expected: {expected_result}, Actual: {result}"


import subprocess

def compare_versions(path):
    # Execute the command "air sandbox info" and capture the output
    command = f"air sandbox info {path}"
    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    stdout, stderr = process.communicate()

    # Extract the values of "latest version" and "sandbox version" from the output
    latest_version = None
    sandbox_version = None
    for line in stdout.splitlines():
        if line.startswith("latest version:"):
            latest_version = line.split(":")[1].strip()
        elif line.startswith("sandbox version:"):
            sandbox_version = line.split(":")[1].strip()

    # Compare the values and return True if they are the same, False otherwise
    return latest_version == sandbox_version

# Example usage:
path = "/path/to/sandbox"
result = compare_versions(path)
print(result)
--------------------------------------

df = df.withColumn("joindate", 
                   when(df["customer_id"].isin("300001", "300002"), "12022023")
                   .otherwise(df["joindate"]))

# Show the updated DataFrame
df.show()
---------------------------------------------------------
def write_dataframe_to_csv(df, file_path, mode="overwrite", header=True):
    """
    Write DataFrame to CSV file.

    Args:
        df (pyspark.sql.DataFrame): DataFrame to write.
        file_path (str): Path to the CSV file.
        mode (str, optional): Specify the mode for saving data. 
            Can be one of "append", "overwrite", "ignore", "error". Defaults to "overwrite".
        header (bool, optional): Whether to write the header in the CSV file. Defaults to True.
    """
    df.write.mode(mode).option("header", header).csv(file_path)

def write_dataframe_to_json(df, file_path, mode="overwrite"):
    """
    Write DataFrame to JSON file.

    Args:
        df (pyspark.sql.DataFrame): DataFrame to write.
        file_path (str): Path to the JSON file.
        mode (str, optional): Specify the mode for saving data. 
            Can be one of "append", "overwrite", "ignore", "error". Defaults to "overwrite".
    """
    df.write.mode(mode).json(file_path)

def write_dataframe_to_parquet(df, file_path, mode="overwrite"):
    """
    Write DataFrame to Parquet file.

    Args:
        df (pyspark.sql.DataFrame): DataFrame to write.
        file_path (str): Path to the Parquet file.
        mode (str, optional): Specify the mode for saving data. 
            Can be one of "append", "overwrite", "ignore", "error". Defaults to "overwrite".
    """
    df.write.mode(mode).parquet(file_path)

# Stop SparkSession
spark.stop()

def load_csv_with_schema(file_path, schema):
    """
    Load CSV file with specified schema.

    Args:
        file_path (str): Path to the CSV file.
        schema (pyspark.sql.types.StructType): Schema to apply.

    Returns:
        pyspark.sql.DataFrame: Loaded DataFrame.
    """
    return spark.read.schema(schema).csv(file_path)

def load_json_with_schema(file_path, schema):
    """
    Load JSON file with specified schema.

    Args:
        file_path (str): Path to the JSON file.
        schema (pyspark.sql.types.StructType): Schema to apply.

    Returns:
        pyspark.sql.DataFrame: Loaded DataFrame.
    """
    return spark.read.schema(schema).json(file_path)

def load_parquet_with_schema(file_path, schema, mode="append"):
    """
    Load Parquet file with specified schema and mode.

    Args:
        file_path (str): Path to the Parquet file.
        schema (pyspark.sql.types.StructType): Schema to apply.
        mode (str, optional): Specify the mode for saving data. 
            Can be one of "append" or "overwrite". Defaults to "append".

    Returns:
        pyspark.sql.DataFrame: Loaded DataFrame.
    """
    return spark.read.schema(schema).parquet(file_path, mode=mode)

# Stop SparkSession
spark.stop()
--------------------------------------------------------------------------

new_df = new_df.filter(~col("array_column")[66]["value"].isin(estimator_df["scenarioID"]))
new_df = new_df.filter(~array_contains(col("array_column")[66]["value"], estimator_df["scenarioID"]))

new_df = new_df.filter(~col("array_column66][""].isin(imator_df["scenarioID"]))

new_df = new_df.join(estimator_df, new_df["array_column"][66]["value"] == estimator_df["scenarioID"], "leftanti")
hi


from pyspark.sql.window import Window
from pyspark.sql.functions import col, row_number

result_df = new_df.join(spark_df, new_df["customerid"] == spark_df["customerid"], "inner")

result_df = result_df.drop(*[col for col in new_df.columns if col not in ["customerid"]])

result_df.show()



dfWithRowNumber.unpersist()

result_df = new_df.join(spark_df, new_df["customerid"] == spark_df["customerid"], "inner")

result_df.show()


from pyspark.sql.window import Window
from pyspark.sql.functions import col, row_number

windowSpec = Window.partitionBy(col("array_column")[66]["value"]).orderBy("salary")

dfWithRowNumber = df.withColumn("row_number", row_number().over(windowSpec))

new_df = dfWithRowNumber.filter(col("row_number") == 1).select(col("array_column")[66]["value"].alias("partition_column"), "customerid", "row_number")

new_df.show()

from pyspark.sql.window import Window
from pyspark.sql.functions import col,_number

windowSpec = Window.By("array_column")[66]["value"]).orderBy("salary")

dfWithRowNumber = df.withColumn("row_number", row_number().over(windowSpec))

dfWithRowNumber.where(col("row_number") == 1).select(col("array_column")[66]["value"], "row_number").show()




import json

# Open the JSON file in read mode
with open('example.json', 'r') as f:
    # Load the JSON data
    data = json.load(f)

# Open the text file in write mode
with open('output.txt', 'w') as outfile:
    # Iterate through the JSON data
    for i, item in enumerate(data):
        # Print the index and JSON data
        print(f'Index: {i}, JSON data: {item["city"]}')

        # Write the index and JSON data to the text file
        outfile.write(f'Index: {i}, JSON data: {item["city"]}\n')
------------

from pyspark.sql.functions import expr, col

index = 50
df1 = df.withColumn("foo", 
  expr("transform(foo, x -> 
    case when array_index(x, "+str(index)+") is null 
         then struct(10 as value, x.value2 as value2) 
         else struct(x.value as value, x.value2 as value2) end)")
)

---++--++++++++++++
df_dc.withColumn('report_date_10', F.date_add(df_dc['report_date'], 10)).show()
from pyspark.sql.functions import col

# Assuming BACS is an array of structs and you want to update the value of BACS[50].value
df = df.withColumn("BACS", 
                   col("BACS").getItem(50).getField("value").cast("int").cast("int").alias("BACS_value"))

from pyspark.sql.functions import rand, when from pyspark.sql.types import DateType from datetime import datetime, timedelta

Generate random customerid between 300000 and 400000
df = df.withColumn("customerid", (rand() * 100000).cast("int") + 300000)

Generate random values for Empstatus (either '05' or '04')
df = df.withColumn("Empstatus", when(rand() < 0.5, '05').otherwise('04'))

Generate random values for BACS (either 1 or 0)
df = df.withColumn("BACS", (rand() < 0.5).cast("int"))

Generate random values for Howner (either 1 or 0)
df = df.withColumn("Howner", (rand() < 0.5).cast("int"))

Generate random dates for Joindate between '2004-01-01' and '2020-06-02'
start_date = datetime(2004, 1, 1) end_date = datetime(2020, 6, 2) date_range = end_date - start_date df = df.withColumn("Joindate", (rand() * date_range.days).cast("int"))
.withColumn("Joindate", (start_date + col("Joindate").cast("int")).cast(DateType()))


Here is the corrected code to generate random customerid, Empstatus, BACS, Howner, and Joindate:

python
Edit
Download
Copy code
from pyspark.sql.functions import rand, when
from pyspark.sql.types import DateType
from datetime import datetime, timedelta

# Generate random customerid between 300000 and 400000
df = df.withColumn("customerid", (rand() * 100000).cast("int") + 300000)

# Generate random values for Empstatus (either '05' or '04')
df = df.withColumn("Empstatus", when(rand() < 0.5, '05').otherwise('04'))

# Generate random values for BACS (either 1 or 0)
df = df.withColumn("BACS", (rand() < 0.5).cast("int"))

# Generate random values for Howner (either 1 or 0)
df = df.withColumn("Howner", (rand() < 0.5).cast("int"))

# Generate random dates for Joindate between '2004-01-01' and '2020-06-02'
start_date = datetime(2004, 1, 1)
end_date = datetime(2020, 6, 2)
date_range = end_date - start_date

# Generate random dates for Joindate between '2004-01-01' and '2020-06-02'
df = df.withColumn("Joindate", (rand() * date_range.days).cast("int")) \
       .withColumn("Joindate", from_unixtime(unix_timestamp("Joindate", "yyyy-MM-dd") + col("Joindate").cast("int"))) \
       .withColumn("Joindate", to_date("Joindate"))
The main issue in the original code was the incorrect usage of rand() to generate random dates. The correct approach is to generate a random number of days between the start and end dates, and then add this number of days to the start date.

Additionally, the from_unixtime() and to_date() functions were used to correctly convert the random number of days to a date format.

The corrected code also includes the necessary import statements for the DateType and from_unixtime() and to_date() functions.

Is this conversation helpful so far?



Explain code generated


The code provided generates a DataFrame with random values for the following columns:

customerid: A random integer between 300000 and 400000.
Empstatus: A random value between '05' and '04'.
BACS: A random integer between 0 and 1.
Howner: A random integer between 0 and 1.
Joindate: A random date between '2004-01-01' and '2020-06-02'.
Here is the step-by-step explanation of the code:

Generate a random customerid between 300000 and 400000:
python
Edit
Download
Copy code
df = df.withColumn("customerid", (rand() * 100000).cast("int") + 300000)
Generate a random value for Empstatus (either '05' or '04'):
python
Edit
Download
Copy code
df = df.withColumn("Empstatus", when(rand() < 0.5, '05').otherwise('04'))
Generate a random value for BACS (either 1 or 0):
python
Edit
Download
Copy code
df = df.withColumn("BACS", (rand() < 0.5).cast("int"))
Generate a random value for Howner (either 1 or 0):
python
Edit
Download
Copy code
df = df.withColumn("Howner", (rand() < 0.5).cast("int"))
Generate a random date for Joindate between '2004-01-01' and '2020-06-02':
python
Edit
Download
Copy code
start_date = datetime(2004, 1, 1)
end_date = datetime(2020, 6, 2)
date_range = end_date - start_date

df = df.withColumn("Joindate", (rand() * date_range.days).cast("int")) \
       .withColumn("Joindate", from_unixtime(unix_timestamp("Joindate", "yyyy-MM-dd") + col("Joindate").cast("int"))) \
       .withColumn("Joindate", to_date("Joindate"))
In this step, a random number of days is generated between 0 and the total number of days between the start and end dates. This random number of days is then added to the start date to generate a random date.

The from_unixtime() function is used to convert the random number of days to a timestamp format. The unix_timestamp() function is used to convert the start date to a timestamp format. The to_date() function is used to convert the timestamp back to a date format.

The final DataFrame will have the specified random values for each column.

from pyspark.sql.functions import rand, when from pyspark.sql.types import DateType from datetime import datetime, timedelta

Generate random customerid between 300000 and 400000
df = df.withColumn("customerid", (rand() * 100000).cast("int") + 300000)

Generate random values for Empstatus (either '05' or '04')
df = df.withColumn("Empstatus", when(rand() < 0.5, '05').otherwise('04'))

Generate random values for BACS (either 1 or 0)
df = df.withColumn("BACS", (rand() < 0.5).cast("int"))

Generate random values for Howner (either 1 or 0)
df = df.withColumn("Howner", (rand() < 0.5).cast("int"))

Generate random dates for Joindate between '2004-01-01' and '2020-06-02'
start_date = datetime(2004, 1, 1) end_date = datetime(2020, 6, 2) date_range = end_date - start_date df = df.withColumn("Joindate", (rand() * date_range.days).cast("int"))
.withColumn("Joindate", (start_date + col("Joindate").cast("int")).cast(DateType()))


Here is the corrected code to generate random customerid, Empstatus, BACS, Howner, and Joindate:

python
Edit
Download
Copy code
from pyspark.sql.functions import rand, when
from pyspark.sql.types import DateType
from datetime import datetime, timedelta

# Generate random customerid between 300000 and 400000
df = df.withColumn("customerid", (rand() * 100000).cast("int") + 300000)

# Generate random values for Empstatus (either '05' or '04')
df = df.withColumn("Empstatus", when(rand() < 0.5, '05').otherwise('04'))

# Generate random values for BACS (either 1 or 0)
df = df.withColumn("BACS", (rand() < 0.5).cast("int"))

# Generate random values for Howner (either 1 or 0)
df = df.withColumn("Howner", (rand() < 0.5).cast("int"))

# Generate random dates for Joindate between '2004-01-01' and '2020-06-02'
start_date = datetime(2004, 1, 1)
end_date = datetime(2020, 6, 2)
date_range = end_date - start_date

# Generate random dates for Joindate between '2004-01-01' and '2020-06-02'
df = df.withColumn("Joindate", (rand() * date_range.days).cast("int")) \
       .withColumn("Joindate", from_unixtime(unix_timestamp("Joindate", "yyyy-MM-dd") + col("Joindate").cast("int"))) \
       .withColumn("Joindate", to_date("Joindate"))
The main issue in the original code was the incorrect usage of rand() to generate random dates. The correct approach is to generate a random number of days between the start and end dates, and then add this number of days to the start date.

Additionally, the from_unixtime() and to_date() functions were used to correctly convert the random number of days to a date format.

The corrected code also includes the necessary import statements for the DateType and from_unixtime() and to_date() functions.

Is this conversation helpful so far?



Explain code generated


The code provided generates a DataFrame with random values for the following columns:

customerid: A random integer between 300000 and 400000.
Empstatus: A random value between '05' and '04'.
BACS: A random integer between 0 and 1.
Howner: A random integer between 0 and 1.
Joindate: A random date between '2004-01-01' and '2020-06-02'.
Here is the step-by-step explanation of the code:

Generate a random customerid between 300000 and 400000:
python
Edit
Download
Copy code
df = df.withColumn("customerid", (rand() * 100000).cast("int") + 300000)
Generate a random value for Empstatus (either '05' or '04'):
python
Edit
Download
Copy code
df = df.withColumn("Empstatus", when(rand() < 0.5, '05').otherwise('04'))
Generate a random value for BACS (either 1 or 0):
python
Edit
Download
Copy code
df = df.withColumn("BACS", (rand() < 0.5).cast("int"))
Generate a random value for Howner (either 1 or 0):
python
Edit
Download
Copy code
df = df.withColumn("Howner", (rand() < 0.5).cast("int"))
Generate a random date for Joindate between '2004-01-01' and '2020-06-02':
python
Edit
Download
Copy code
start_date = datetime(2004, 1, 1)
end_date = datetime(2020, 6, 2)
date_range = end_date - start_date

df = df.withColumn("Joindate", (rand() * date_range.days).cast("int")) \
       .withColumn("Joindate", from_unixtime(unix_timestamp("Joindate", "yyyy-MM-dd") + col("Joindate").cast("int"))) \
       .withColumn("Joindate", to_date("Joindate"))
In this step, a random number of days is generated between 0 and the total number of days between the start and end dates. This random number of days is then added to the start date to generate a random date.

The from_unixtime() function is used to convert the random number of days to a timestamp format. The unix_timestamp() function is used to convert the start date to a timestamp format. The to_date() function is used to convert the timestamp back to a date format.

The final DataFrame will have the specified random values for each column.
_---------+
from pyspark.sql.functions import rand, when
from pyspark.sql.types import DateType
from datetime import datetime, timedelta

# Generate random customerid between 300000 and 400000
df = df.withColumn("customerid", (rand() * 100000).cast("int") + 300000)

# Generate random values for Empstatus (either '05' or '04')
df = df.withColumn("Empstatus", when(rand() < 0.5, '05').otherwise('04'))

# Generate random values for BACS (either 1 or 0)
df = df.withColumn("BACS", (rand() < 0.5).cast("int"))

# Generate random values for Howner (either 1 or 0)
df = df.withColumn("Howner", (rand() < 0.5).cast("int"))

# Generate random dates for Joindate between '2004-01-01' and '2020-06-02'
start_date = datetime(2004, 1, 1)
end_date = datetime(2020, 6, 2)
date_range = end_date - start_date
df = df.withColumn("Joindate", (rand() * date_range.days).cast("int")) \
       .withColumn("Joindate", (start_date + col("Joindate").cast("int")).cast(DateType()))
----+-+-----------------
from pyspark.sql.functions import rand, when
from pyspark.sql.types import DateType
from datetime import datetime, timedelta

# Generate random customerid between 300000 and 400000
df = df.withColumn("customerid", (rand() * 100000).cast("int") + 300000)

# Generate random values for Empstatus (either '05' or '04')
df = df.withColumn("Empstatus", when(rand() < 0.5, '05').otherwise('04'))

# Generate random values for BACS (either 1 or 0)
df = df.withColumn("BACS", (rand() < 0.5).cast("int"))

# Generate random values for Howner (either 1 or 0)
df = df.withColumn("Howner", (rand() < 0.5).cast("int"))

# Generate random dates for Joindate between '2004-01-01' and '2020-06-02'
start_date = datetime(2004, 1, 1)
end_date = datetime(2020, 6, 2)
date_range = end_date - start_date

# Generate random dates for Joindate between '2004-01-01' and '2020-06-02'
df = df.withColumn("Joindate", (rand() * date_range.days).cast("int")) \
       .withColumn("Joindate", from_unixtime(unix_timestamp("Joindate", "yyyy-MM-dd") + col("Joindate").cast("int"))) \
       .withColumn("Joindate", to_date("Joindate"))
-----------------------------
To update the joindate column for all records in a DataFrame, you can use the withColumn() function to create a new DataFrame with the updated joindate values. Here's how you can do it:from pyspark.sql import SparkSession
from pyspark.sql.functions import lit

# Create a SparkSession
spark = SparkSession.builder \
    .appName("UpdateJoinDateColumn") \
    .getOrCreate()

# Assuming 'df' is your DataFrame and 'new_join_date' is the new join date value
new_join_date = "2024-02-08"  # Example new join date

# Update the 'joindate' column with the new join date value for all records
updated_df = df.withColumn("joindate", lit(new_join_date))

# Show the updated DataFrame
updated_df.show()

# Stop the SparkSession
spark.stop()Replace "2024-02-08" with the desired new join date value.In this code, lit(new_join_date) creates a column with a constant value equal to the specified new join date. Then, withColumn("joindate", lit(new_join_date)) updates the joindate column in the DataFrame df with this new constant value, effectively updating the joindate for all records in the DataFrame.
-------------------------------
def parse_json(json_str):
    try:
        if not json_str:
            return ""
        parsed_json = json.loads(json_str)
        # Convert numeric values back to strings
        for key, value in parsed_json.items():
            if isinstance(value, (int, float)) and str(value).isdigit():
                parsed_json[key] = str(value)
        return parsed_json
    except (json.JSONDecodeError, TypeError):
        return {}
-----------------
import pandas as pd
import json

# Define a function to convert the JSON string to a JSON object
def parse_json(json_str):
    try:
        return json.loads(json_str)
    except (json.JSONDecodeError, TypeError):
        return {}

# Read the CSV file, specifying the converter function for the columns with JSON structures
columns_with_json = ['callcredit', 'other_column1', 'other_column2', ..., 'other_column10']
converters = {col: parse_json for col in columns_with_json}
df = pd.read_csv('your_csv_file.csv', converters=converters)

# Convert DataFrame to JSON
json_data = df.to_json(orient='records')

# Write JSON data to a file
with open('output.json', 'w') as json_file:
    json_file.write(json_data)

# Now json_data contains the JSON representation of your DataFrame
# and it's saved in a file named 'output.json'
-----------------------------
import json

# Read JSON data from a file
with open('data.json') as f:
    data = json.load(f)

# Assuming 'callcredit' is the column containing the struct data as a string
if 'callcredit' in data:
    try:
        # Parse the string representation into a Python dictionary
        callcredit_dict = json.loads(data['callcredit'])
        
        # Update the value associated with 'callcredit' to be the parsed dictionary
        data['callcredit'] = callcredit_dict
    except json.JSONDecodeError:
        # Handle the case where the value is not a valid JSON string
        print("Value in 'callcredit' is not a valid JSON string.")

    # Write the updated JSON data back to the file
    with open('data.json', 'w') as f:
        json.dump(data, f, indent=4)  # Use indent for pretty printing (optional)
---------------------------------------
import csv

# Define your scoring function here
def calculate_score(a, b, c, d, e, f, g):
    # Example scoring logic, replace with your own
    score = a + b + c + d + e + f + g  # Just summing up the values as an example
    return score

def main():
    # Open the pipe-delimited file
    with open('data.txt', 'r') as file:
        reader = csv.reader(file, delimiter='|')
        
        # Skip header if present
        next(reader)
        
        # Iterate over each row
        for row in reader:
            # Assign variables from 'a' to 'g' for the 8 variables
            a, b, c, d, e, f, g = map(int, row)
            
            # Calculate score for the row
            score = calculate_score(a, b, c, d, e, f, g)
            
            # Do something with the score (print it for now)
            print("Score for row {}: {}".format(row, score))

if __name__ == "__main__":
    main()
---------------------------------
import json

# Read JSON data from a file
with open('data.json') as f:
    data = json.load(f)
#if 'key_to_update' not in data or data['key_to_update'] is None:
# Check if the value associated with the key 'key_to_update' is "NULL"
if data.get('key_to_update') == "NULL":
    # Update the value associated with the key 'key_to_update'
    data['key_to_update'] = 'new_value'

    # Write the updated JSON data back to the file
    with open('data.json', 'w') as f:
        json.dump(data, f, indent=4)  # Use indent for pretty printing (optional)
-----------------------------------
#!/bin/bash

# Step 1: Define the Beeline connection string
beeline_connection="jdbc:hive2://your_hive_server:10000/default"

# Step 2: Run the beeline command to execute new.hql and save output to results.csv
beeline_command="beeline -u $beeline_connection -f new.hql > results.csv"

# Execute the beeline command
$beeline_command

# Step 3: Check if results.csv filesize is less than 3mb
filesize=$(stat -c %s results.csv)
filesize_mb=$((filesize / 1024 / 1024))

if [ $filesize_mb -lt 3 ]; then
    # If filesize is less than 3mb, send the file via mail using mailx
    mailx -s "Results File" your_email@example.com < results.csv
else
    # If filesize is greater than or equal to 3mb, send mail notification
    mailx -s "Results File Size Warning" your_email@example.com <<< "The results file size is large. Please collect it from location."
fi
--------------------------------------------
import json

# Read JSON data from a file
with open('data.json', 'r+') as f:
    data = json.load(f)

    # Loop through each record (assuming data is a list)
    for record in data:
        # Update the value associated with the key 'joindate' for each record
        record['joindate'] = 'new_value'

    # Go to the beginning of the file
    f.seek(0)

    # Write the updated JSON data back to the file
    json.dump(data, f, indent=4)  # Use indent for pretty printing (optional)

    # Truncate any remaining content (in case the new data is shorter than the original)
    f.truncate()
--------------------------
import json

# Read JSON data from a file
with open('data.json') as f:
    data = json.load(f)

# Update the value associated with the key 'key_to_update'
data['key_to_update'] = 'new_value'

# Write the updated JSON data back to the file
with open('data.json', 'w') as f:
    json.dump(data, f, indent=4)  # Use indent for pretty printing (optional)
--------------------
import json

# Open the JSON file
with open('data.json') as f:
    # Load JSON data
    data = json.load(f)

# Now you can work with the 'data' variable which contains your JSON data
print(data)
-----++----------------------
import os
from datetime import datetime
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder \
    .appName("DataFrameToJson") \
    .getOrCreate()

def write_df_to_json(df, output_path):
    """
    Write DataFrame to a JSON file with the current date and time in the file name
    at the specified path.

    Parameters:
        df (DataFrame): The DataFrame to write to the JSON file.
        output_path (str): The directory path where the JSON file will be saved.
    """
    # Get the current date and time
    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    
    # Generate the file name with the current date and time
    file_name = f"{current_datetime}.json"
    
    # Concatenate the output path with the file name
    json_file_path = os.path.join(output_path, file_name)
    
    # Write DataFrame to JSON file
    df.write.json(json_file_path)

# Example usage:
# Assuming 'df' is your DataFrame
# Path where the JSON file will be saved
output_path = "path/to/save/json"

# Write DataFrame to JSON file with current date and time in the file name
write_df_to_json(df, output_path)

# Stop the SparkSession
spark.stop()
-----------------------

data = [("Java", "20000"), ("Python", "100000"), ("Scala", "3000")]
>>> df = spark.createDataFrame(data)
>>> df.show()
---------------------

import json

def analyze_json(json_file, key):
    with open(json_file, 'r') as f:
        data = json.load(f)
        if key in data:
            return data[key]
        else:
            return "Key not found in JSON."

# Example usage:
json_file = 'data.json'
key_to_find = 'example_key'
value = analyze_json(json_file, key_to_find)
print("Value for key '{}': {}".format(key_to_find, value))
-------------------------------
#!/bin/bash

# Run your command and capture the output
output=$(your_command_here)

# Check if the output is empty or not
if [ -n "$output" ]; then
    echo "Command successful"
else
    echo "Command failed"
fi
-------------------------------
import pyarrow as pa
import pyarrow.parquet as pq
import json

# Assuming your JSON file is named 'input.json'
json_file_path = 'input.json'

# Read JSON file into a Python dict
with open(json_file_path, 'r') as json_file:
    json_data = json.load(json_file)

# Convert the dict to a pyarrow Table
table = pa.Table.from_pandas(pd.DataFrame(json_data))

# Assuming you want to convert to a Parquet file named 'output.parquet'
parquet_file_path = 'output.parquet'

# Write the pyarrow Table to Parquet file
pq.write_table(table, parquet_file_path)

print(f"Conversion complete. JSON file '{json_file_path}' converted to Parquet file '{parquet_file_path}'.")

# Show Schema
print("\nSchema:")
print(table.schema)

# Show Data
parquet_table = pq.read_table(parquet_file_path)
parquet_data = parquet_table.to_pandas()
print("\nData:")
print(parquet_data)
----------++++----------------------
"""from flask import Flask

app = Flask(__name__)

@app.route('/')
def hello():
    return 'Hello, World!'

if __name__ == '__main__':
    app.run(debug=True, port=8080)"""

# app.py
import streamlit as st
import subprocess

def print_message(message):
    st.write(message)

def run_os_commands():
    commands = [
        'start "" "C:\\Program Files (x86)\\Cisco\\Cisco AnyConnect Secure Mobility Client\\vpnui.exe"',
        'start "" "C:\\Program Files (x86)\\Ping Identity\\PingID\\PingID.exe"',
        'start "" "C:\\Program Files\\Google\\Chrome\\Application\\chrome_proxy.exe" --profile-directory=Default --app-id=kloeccplkmphhfhfkkmjnnbopencohcp'
    ]
    
    for command in commands:
        try:
            subprocess.run(command, shell=True)
        except Exception as e:
            st.error(f"Error running command: {e}")

def open_notepad():
    try:
        subprocess.run(['notepad.exe'])
    except Exception as e:
        st.error(f"Error: {e}")

def main():
    st.title("Hello, Streamlit!")
    st.write("This is a simple Streamlit app.")
    st.write("Bhakalya")
    st.title("Streamlit Button Example")

    # Add a button to the app
    button_clicked = st.button("Open Notepad")

    # Check if the button is clicked
    if button_clicked:
        st.write("Button clicked!")
        open_notepad()
        run_os_commands()
    
    st.title("Streamlit Sidebar Buttons Example")

    # Sidebar options
    option1_button = st.sidebar.button("Option 1")
    option2_button = st.sidebar.button("Option 2")
    option3_button = st.sidebar.button("Option 3")

    # Handle button clicks
    if option1_button:
        print_message("You clicked Option 1!")
    elif option2_button:
        print_message("You clicked Option 2!")
    elif option3_button:
        print_message("You clicked Option 3!")

if __name__ == '__main__':
    main()
---------------------------------------------
Query for NUC 
SELECT
  column_name,
  CASE
    WHEN INSTR(column_name, '.') > 0 THEN
      CONCAT(
        CASE
          WHEN SUBSTRING(column_name, INSTR(column_name, '.') - 1, 1) <> '1'
          THEN SUBSTRING(column_name, 1, INSTR(column_name, '.') - 1) || '1'
          ELSE SUBSTRING(column_name, 1, INSTR(column_name, '.') - 1)
        END,
        SUBSTRING(column_name, INSTR(column_name, '.'), LENGTH(column_name))
      )
    ELSE column_name
  END AS modified_value
FROM
  your_table
WHERE
  column_name IS NOT NULL;
------------------------
list_sum = 0
for num in mylist:
	list_sum = list_sum + num
print(list_sum)
----------------------------------------------
import pandas as pd

csv_file_path = 'your_input_file.csv'
json_file_path = 'output.json'

# Read CSV file into a pandas DataFrame
df = pd.read_csv(csv_file_path)

# Convert DataFrame to JSON and save to a file
df.to_json(json_file_path, orient='records', indent=2)

print(f'Conversion successful. JSON file saved at {json_file_path}')







-----+----------+-+
#!/bin/bash

key="nram_od"
value="3"

if [ "$key" = "nram_od" ]; then
    if [ "$value" = "1" ]; then
        echo "some_line"
    elif [ "$value" = "2" ]; then
        echo "some_line"
    elif [ "$value" = "3" ]; then
        echo "some_line1"
        echo "some_line2"
    fi
elif [ "$key" = "Nuk" ]; then
    if [ "$value" != "-999.0" ]; then
        echo "some_line"
    fi
fi
---------------------------
#!/bin/bash

# Additional condition
if [ "$New_Marker" -eq 1 ] && [[ ! "0762 4208 4199" =~ $Code ]]; then
    # If Limit is a decimal value, replace the last digit before the decimal with '1'
    Limit_str=$(echo "$Limit" | tr -d '\n')
    if [[ "$Limit_str" == *.* ]]; then
        decimal_part=$(echo "$Limit_str" | cut -d'.' -f1)
        fractional_part=$(echo "$Limit_str" | cut -d'.' -f2)
        Limit="${decimal_part%?}1${fractional_part:+.$fractional_part}"
    else
        # If Limit is a natural number, use "${Limit%?}1"
        Limit="${Limit%?}1"
    fi
fi


------------------------------++
prasadswami76@gmail.com
Create a shell script to check whether Content from one list is present in another list or not

Sure, here's a shell script that will check if the content from one list is present in another list or not:

bash
Copy code
#!/bin/bash

# Define two lists
list1=("apple" "banana" "cherry" "date")
list2=("cherry" "date" "fig" "grape")

# Loop through each element in list1
for i in "${list1[@]}"
do
    # Check if the element is present in list2
    if [[ "${list2[*]}" =~ "$i" ]]
    then
        echo "$i is present in list2"
    else
        echo "$i is not present in list2"
    fi
done
In this script, we have defined two lists, list1 and list2, and we are looping through each element in list1. For each element in list1, we are checking if it is present in list2 using the =~ operator in the if statement. If the element is present in list2, we print a message saying it is present, otherwise, we print a message saying it is not present.

You can modify the script by replacing the elements in list1 and list2 with your own lists, and run it on your local machine by saving it in a file with a .sh extension, making it executable using chmod +x script.sh, and running it with ./script.sh.




prasadswami76@gmail.com
In the first list I have policycodes in random order.. In second list I have policycodes in arranged order.. I want to check if policycodes from first list is present in second list or not.. if yes then arrange thoes in second list order

Sure, here's an updated script that will check if the policy codes from the first list are present in the second list and if so, arrange them in the same order as the second list:

bash
Copy code
#!/bin/bash

# Define two lists
list1=("PC002" "PC001" "PC005" "PC003")
list2=("PC001" "PC002" "PC003" "PC004" "PC005")

# Loop through each element in list1
for i in "${list1[@]}"
do
    # Check if the element is present in list2
    if [[ "${list2[*]}" =~ "$i" ]]
    then
        # If present, add it to a new array in the order of list2
        new_list+=("$i")
    fi
done

# Print the new list
echo "${new_list[@]}"



/bin/bash

# Define two lists

list1=("C0001" "PC002" "PC001" "PC005" "PC003")

list2=("PC001" "PC002" "PC003" "PC004" "PC005")

# Get the customer id from the first list

customer_id=${list1[0]}

# Loop through each element in list1 starting from index 1

for i in "${list1[@]:1}"

do

    # Check if the element is present in list2

    if [[ "${list2[*]}" =~ "$i" ]]

    then

        # If present, add it to a new array in the order of list2

        new_list+=("$i")

    fi

done

# Put the customer id at the beginning of the new list

new_list=("$customer_id" "${new_list[@]}")

# Print the first five elements of the new list

echo "${new_list[@]:0:5}"

In this script, we have defined two lists, list1 and list2, and we are getting the customer id from the first element of list1. We then loop through each element in list1 starting from index 1, and for each element that is present in list2, we add it to a new array new_list in the order of list2.

Finally, we put the customer id at the beginning of new_list and print the first five elements of new_list.

You can modify the script by replacing the elements in list1 and list2 with your own lists and run it on your local machine by saving it in a file with a .sh extension, making it executable using chmod +x script.sh, and running it with ./script.sh.

prasadswami76@gmail.com

we add it to a new array new_list in the order of list2 please explain how code will achieve this

Sure, here's how the script will achieve adding the policy codes from list1 that are present in list2 to a new array new_list in the order of list2:

We define an empty new_list array at the beginning of the script: new_list=()

Inside the loop that iterates over the elements of list1, we use the =~ operator to check if the current element is present in list2.

If the element is present in list2, we add it to the new_list array using the += operator: new_list+=("$i"). This adds the current element to the end of the new_list array.

At the end of the loop, new_list contains all the policy codes from list1 that are present in list2, but they may not be in the order of list2.

To put the policy codes in the order of list2, we can modify the loop to iterate over the elements of list2 instead of list1, and use another loop to check if each element of list2 is present in list1 and add it to new_list in the order of list2. However, this would require a nested loop and could be less efficient if list1 and list2 are large.




we can modify the loop to iterate over the elements of list2 instead of list1, and use another loop to check if each element of list2 is present in list1 and add it to new_list in the order of 



Instead, we can use the += operator to add elements to new_list in the order of list2 by using the ${new_list[*]} syntax to expand new_list as a space-separated string, and then using the += operator to append the current element to new_list in the order of list2: new_list+=("$i"). This adds the current element to the end of new_list, but in the order of list2 because we are appending it after all the elements of new_list that are already in list2 order.

Overall, this approach allows us to add the policy codes from list1 to new_list in the order of list2 without the need for a nested loop.














ash

Copy code

#!/bin/bash

# Define two lists

list1=("PC002" "PC001" "PC005" "PC003")

list2=("PC001" "PC002" "PC003" "PC004" "PC005")

# Create a new empty array to hold the policy codes in the order of the second list

new_list=()

# Loop through each element in list2

for i in "${list2[@]}"

do

    # Loop through each element in list1 to find the index of the matching policy code

    for j in "${!list1[@]}"

    do

        if [[ "$i" == "${list1[$j]}" ]]

        then

            # If a matching policy code is found, add it to the new list in the order of list2

            new_list+=("${list1[$j]}")

            break

        fi

    done

done

# Print the new array

echo "${list1[0]} ${new_list[@]:0:4}"





!/bin/bash

# Define two lists

list1=("C001" "PC002" "PC001" "PC005" "PC003")

list2=("PC001" "PC002" "PC003" "PC004" "PC005")

# Extract the customer ID from list1

customer_id="${list1[0]}"

# Remove the first element of list1

list1=("${list1[@]:1}")

# Create a new empty array to hold the policy codes in the order of the second list

new_list=()

# Loop through each element in list2

for i in "${list2[@]}"

do

    # Loop through each element in list1 to find the index of the matching policy code

    for j in "${!list1[@]}"

    do

        if [[ "$i" == "${list1[$j]}" ]]

        then

            # If a matching policy code is found, add it to the new list in the order of list2

            new_list+=("${list1[$j]}")

            break

        fi

    done

done

# Print the customer ID followed by the first four policy codes in the order of list2

echo "$customer_id ${new_list[@]:0:4}"



#!/bin/bash

# Define the path to the CSV file

csv_file="path/to/file.csv"

# Define the second list

list2=("PC001" "PC002" "PC003" "PC004" "PC005")

# Create the target CSV file with header

echo "Customer ID, Policy Code 1, Policy Code 2, Policy Code 3, Policy Code 4" > target.csv

# Loop through each line in the CSV file

while read -r line

do

    # Extract the customer ID and policy codes from the line

    IFS=',' read -ra fields <<< "$line"

    customer_id="${fields[0]}"

    list1=("${fields[@]:1}")

    # Remove the customer ID from list1

    list1=("${list1[@]:1}")

    # Create a new empty array to hold the policy codes in the order of the second list

    new_list=()

    # Loop through each element in list2

    for i in "${list2[@]}"

    do

        # Loop through each element in list1 to find the index of the matching policy code

        for j in "${!list1[@]}"

        do

            if [[ "$i" == "${list1[$j]}" ]]

            then

                # If a matching policy code is found, add it to the new list in the order of list2

                new_list+=("${list1[$j]}")

                break

            fi

        done

    done

    # Create a new line for the target CSV file

    new_line="$customer_id,${new_list[@]:0:4}"

    

    # Add the new line to the target CSV file

    echo "$new_line" >> target.csv

done < "$csv_file"

In this modified script, we first create the target CSV file with a header using the echo command: echo "Customer ID, Policy Code 1, Policy Code 2, Policy Code 3, Policy Code 4" > target.csv. This creates a new file called target.csv and writes the header to it.

We then modify the loop to create a new line for the target CSV file using new_line="$customer_id,${new_list[@]:0:4}", which concatenates the customer ID with the first four policy codes in the order of list2 separated by commas. Finally, we add the new line to the target CSV file using echo "$new_line" >> target.csv.

With this modification, the script will write the customer ID and first four policy codes in the order of list2 for each line in the input CSV file to a new CSV file called target.csv.

ChatGPT Mar 14 Version. Free Research Preview. Our goal is to make AI systems more natural and safe to inter











import csv

# Define the path to the CSV file

csv_file = "path/to/file.csv"

# Define the second list

list2 = ["PC001", "PC002", "PC003", "PC004", "PC005"]

# Create the target CSV file with header

with open("target.csv", "w") as target_file:

    writer = csv.writer(target_file)

    writer.writerow(["Customer ID", "Policy Code 1", "Policy Code 2", "Policy Code 3", "Policy Code 4"])

    # Loop through each line in the CSV file

    with open(csv_file, "r") as file:

        reader = csv.reader(file)

        for fields in reader:

            # Extract the customer ID and policy codes from the line

            customer_id = fields[0]

            list1 = fields[1:]

            # Remove the customer ID from list1

            list1 = list1[1:]

            # Create a new empty list to hold the policy codes in the order of the second list

            new_list = []

            # Loop through each element in list2

            for i in list2:

                # Loop through each element in list1 to find the index of the matching policy code

                for j, code in enumerate(list1):

                    if i == code:

                        # If a matching policy code is found, add it to the new list in the order of list2

                        new_list.append(code)

                        break

                else:

                    # If no matching policy code is found, add a padding of 0 to the new list

                    new_list.append("0")

            # Create a new row for the target CSV file

            new_row = [customer_id] + new_list[:4]

            # Add the new row to the target CSV file

            writer.writerow(new_row)

Note that in Python, we use the csv module to read and write CSV files, and we use a with statement to ensure that the files are properly closed after we are done with them. We also use lists instead of arrays to store the policy codes, and we use the enumerate() function to loop through a list while keeping track of the index.










Run Command from Variable

prasadswami76@gmail.com

I have one command stored as a variable I want to run that command in shell script how to do that

To run a command stored in a variable in a shell script, you can use the eval command in Bash. The eval command allows you to evaluate a command that is stored in a string variable as if it were a regular command.

Here's an example:

bash

Copy code

#!/bin/bash

# Store the command in a variable

cmd="ls -l"

# Use eval to run the command

eval $cmd

In this example, the ls -l command is stored in the cmd variable. The eval command is then used to run the command stored in the cmd variable.

Note that using eval can be dangerous if the contents of the variable are not properly sanitized, as it can execute arbitrary commands. Make sure that the variable only contains a valid command that you want to run.







You're right. Here's an updated version of the script that can handle a variable number of CSV files provided as a single argument, including the case where only one CSV file is provided:

bash

Copy code

#!/bin/bash

# check that one argument has been provided

if [ "$#" -ne 1 ]; then

    echo "Usage: $0 <csv1 csv2 csv3 ...>"

    exit 1

fi

# split the argument into an array of CSV filenames

IFS=' ' read -r -a files <<< "$1"

# check that all files are CSV files

for file in "${files[@]}"; do

    if [ ! -f "$file" ] || [ "${file##*.}" != "csv" ]; then

        echo "$file is not a CSV file"

        exit 1

    fi

done

# append all CSV files to the first one

first_file="${files[0]}"

for file in "${files[@]:1}"; do

    cat "$file" >> "$first_file"

done

Save this script to a file, for example append_csv.sh, and make it executable with the command chmod +x append_csv.sh.

To use the script, run it with the names of the CSV files as a single argument separated by spaces, like this:

bash

Copy code

./append_csv.sh "csv1.csv csv2.csv csv3.csv"

The script will split the argument into an array of CSV filenames, check that all files are CSV files, and append the contents of all CSV files to the first one in the order they are provided. If only one CSV file is provided, the script will simply exit without appending anything.

ChatGPT Mar 14 Version. Free Rese




!/bin/bash

# Set path to CSV file
csv_file="table_names.csv"

# Initialize associative array to store table names and counts
declare -A counts

# Read table names from CSV file and execute Hive queries
while read -r table_name
do
    # Define Hive query using table name
    hive_query="SELECT COUNT(*) FROM $table_name;"

    # Execute Hive query and store count in associative array
    count=$(hive -e "$hive_query" | tail -1)
    counts[$table_name]=$count

    echo "Counted $count records in $table_name"
done < "$csv_file"

# Print table names and their corresponding counts
echo "Table Counts:"
for table_name in "${!counts[@]}"
do
    count=${counts[$table_name]}
    echo "$table_name: $count"
done




@echo off

set /p email=Enter your email address:
echo Your email address is %email%
echo Your email address is %email% > output.txt


:menu
cls
echo Select an option:
echo 1. Option 1
echo 2. Option 2
echo 3. Option 3
echo 4. Option 4

set /p choice=Enter your choice (1-4):

if "%choice%"=="1" (
  echo You have selected Option 1
  echo You have selected Option 1 > output.txt
) else if "%choice%"=="2" (
  echo You have selected Option 2
  echo You have selected Option 2 > output.txt
) else if "%choice%"=="3" (
  echo You have selected Option 3
  echo You have selected Option 3 > output.txt
) else if "%choice%"=="4" (
echo You have selected Option 4
  echo You have selected Option 4 > output.txt
) else (
  echo Invalid choice. Please try again.
  pause
  goto menu
)



{(1, 1, 1), (1, 1, 0), (1, 1, NULL), (1, 0, 1), (1, 0, 0), (1, 0, NULL), (1, NULL, 1), (1, NULL, 0), (1, NULL, NULL), (0, 1, 1), (0, 1, 0), (0, 1, NULL), (0, 0, 1), (0, 0, 0), (0, 0, NULL), (0, NULL, 1), (0, NULL, 0), (0, NULL, NULL), (NULL, 1, 1), (NULL, 1, 0), (NULL, 1, NULL), (NULL, 0, 1), (NULL, 0, 0), (NULL, 0, NULL), (NULL, NULL, 1), (NULL, NULL, 0), (NULL, NULL, NULL)}
{(1, 1, 1), (1, 1, 0), (1, 1, NULL), (1, 0, 1), (1, 0, 0), (1, 0, NULL), (1, NULL, 1), (1, NULL, 0), (1, NULL, NULL), (0, 1, 1), (0, 1, 0), (0, 1, NULL), (0, 0, 1), (0, 0, 0), (0, 0, NULL), (0, NULL, 1), (0, NULL, 0), (0, NULL, NULL), (NULL, 1, 1), (NULL, 1, 0), (NULL, 1, NULL), (NULL, 0, 1), (NULL, 0, 0), (NULL, 0, NULL), (NULL, NULL, 1), (NULL, NULL, 0), (NULL, NULL, NULL)}




@echo off
setlocal enabledelayedexpansion

set "infile=input.txt"
set "outfile=output.csv"

if exist "%outfile%" del "%outfile%"

for /f "tokens=* delims=" %%a in ('type "%infile%"') do (
  set "line=%%a"
  set "line=!line:	=,!"
  echo !line!>>"%outfile%"
)

echo "Conversion complete."

option batch abort
option confirm off
open sftp://username:password@example.com

# File 1
if exist C:\local\file1.txt (
    put C:\local\file1.txt /remote/path/file1.txt
    echo sh shellscript.sh > output.txt
) else (
    echo File 1 not found
)

# File 2
if exist C:\local\file2.txt (
    put C:\local\file2.txt /remote/path/file2.txt
    echo sh shellscript.sh > output.txt
) else (
    echo File 2 not found
)

# File 3
if exist C:\local\file3.txt (
    put C:\local\file3.txt /remote/path/file3.txt
    echo sh shellscript.sh > output.txt
) else (
    echo File 3 not found
)

exit



#Local outbound location
lcd C:\Outbound

# Download filenames.txt
get /remote/path/filenames.txt

# Read filenames.txt and transfer each file
option transfer binary
fileopen handle filenames.txt
while !feof handle
    filegets remotePath handle
    if !feof handle
        get %remotePath%
    end
end
fileclose handle

# Delete filenames.txt from the local machine
!del filenames.txt

exit


@echo off
set /p answer="Do you want to eat ice cream? (yes/no): "
if /i "%answer%"=="yes" (
    set /p quantity="How many ice creams do you want to eat? "
    echo You want to eat %quantity% ice cream(s).
) else (
    echo That's okay.
)

setlocal enabledelayedexpansion

set "infile=input.txt"
set "outfile=output.csv"

if exist "%outfile%" del "%outfile%"

for /f "tokens=* delims=" %%a in ('type "%infile%"') do (
  set "line=%%a"
  set "line=!line:	=,!"
  echo !line!>>"%outfile%"
)

echo "Conversion complete."

------------------------------

The error message "relaying denied from 10.122.6.125" typically indicates that the SMTP server is configured to reject relayed email messages from the IP address 10.122.6.125. Here's some information to help you understand this error:

Relaying in SMTP:

Relaying refers to the process of sending an email through one SMTP server to another, where the second server delivers the email to its intended recipient.
To prevent misuse and spam, SMTP servers are often configured to deny relaying by default.
Relaying Denied Error:

When you receive the "relaying denied" error message, it means that the SMTP server receiving the email (possibly the recipient's server) has determined that the server originating the email (10.122.6.125 in this case) is not allowed to relay messages through it.
This restriction is in place to prevent unauthorized or unsolicited emails from being relayed through the server.
Possible Causes and Solutions:

Misconfiguration: Ensure that the email client or application you're using is correctly configured to send emails through the appropriate SMTP server, and that the server's settings permit relaying.
Authentication: Some SMTP servers require authentication before allowing relaying. Ensure that you have provided the necessary authentication credentials (username and password) for the SMTP server.
IP restriction: The SMTP server may have IP-based restrictions in place, allowing relaying only from specific IP addresses or ranges. Check with the server administrator to verify if your IP address (10.122.6.125) is allowed for relaying.
SMTP server policy: The server's policy might simply disallow relaying for security reasons. In such cases, you may need to relay emails through an authorized SMTP server or contact the server administrator for further assistance.
It's worth noting that resolving relaying issues often requires administrative access and configuration changes on the SMTP server. If you're not the administrator of the server or lack the necessary privileges, it's recommended to reach out to the appropriate IT department or the server administrator for further assistance in resolving the relaying denied error.

------------------------------------------------------------------
#!/bin/bash

input_file="input.txt"
output_file="output.csv"

awk 'BEGIN { FS = "|\\^"; OFS = "," } {
  for (i = 1; i <= NF; i++) {
    printf "%s%s", $i, (i < NF ? OFS : "")
  }
  print ""
}' "$input_file" > "$output_file"
---------------------------------------------------
$dummyshell = New-Object -com "Wscript.shell"
$dummyshell.sendkeys(".")
---+++++++---+---+---------
curl --request GET \ --url https:///rest/api/3/issue/ \ --user : \ --header 'Accept: application/json'
_-------------------------------
#!/bin/bash

# List of API endpoints to check
endpoints=(
    "https://api.example.com/endpoint1"
    "https://api.example.com/endpoint2"
    "https://api.example.com/endpoint3"
)

# Create the HTML report file
report_file="api_health_report.html"
echo "<html><body>" > "$report_file"

# Iterate over the endpoints
for endpoint in "${endpoints[@]}"; do
    echo "Checking endpoint: $endpoint"
    
    # Send a GET request to the endpoint and capture the response
    response=$(curl -s -o /dev/null -w "%{http_code}" "$endpoint")

    # Check the HTTP status code
    if [[ "$response" -eq 200 ]]; then
        echo "Endpoint is running fine."
        status_msg="Endpoint is running fine."
        status_color="green"
    else
        echo "Endpoint is not running. HTTP status code: $response"
        status_msg="Endpoint is not running."
        status_color="red"
    fi

    # Add the endpoint, status, and response code to the HTML report
    echo "<p><strong>Endpoint:</strong> $endpoint</p>" >> "$report_file"
    echo "<p><strong>Status:</strong> <span style=\"color: $status_color;\">$status_msg</span></p>" >> "$report_file"
    echo "<p><strong>Response Code:</strong> $response</p>" >> "$report_file"
    echo "<hr>" >> "$report_file"

    echo "------------------------------"
done

# Close the HTML report
echo "</body></html>" >> "$report_file"

echo "HTML report generated: $report_file"
----------------------------------------------------------
(
  echo "Subject: $email_subject"
  echo "MIME-Version: 1.0"
  echo "Content-Type: text/html"
  echo "Content-Disposition: inline"
  echo "Content-Transfer-Encoding: 7bit"
  cat "$report_file"
) | 
---------------------------------------------------------------------
#!/bin/bash

# List of API endpoints to check
endpoints=(
    "https://api.example.com/endpoint1"
    "https://api.example.com/endpoint2"
    "https://api.example.com/endpoint3"
)

# Create the report file
report_file="api_health_report.txt"

# Function to display a table row
print_table_row() {
    printf "| %-40s | %-20s |\n" "$1" "$2"
}

# Function to display a table separator
print_table_separator() {
    printf "+%s+%s+\n" "------------------------------------------" "----------------------"
}

# Generate the report
echo "API Health Check Report" > "$report_file"
echo "" >> "$report_file"
print_table_separator >> "$report_file"
print_table_row "Endpoint" "Status" >> "$report_file"
print_table_separator >> "$report_file"

for endpoint in "${endpoints[@]}"; do
    echo "Checking endpoint: $endpoint"
    
    # Send a GET request to the endpoint and capture the response
    response=$(curl -s -o /dev/null -w "%{http_code}" "$endpoint")

    # Check the HTTP status code
    if [[ "$response" -eq 200 ]]; then
        print_table_row "$endpoint" "Running fine" >> "$report_file"
    else
        print_table_row "$endpoint" "Not running (HTTP $response)" >> "$report_file"
    fi

    print_table_separator >> "$report_file"
done

# Email sending configuration
email_subject="API Health Check Report"
recipient_email="example@example.com"
sender_email="sender@example.com"
smtp_server="smtp.example.com"
smtp_port="587"
smtp_username="your_username"
smtp_password="your_password"

# Send the email with the report file as an attachment
echo "Sending email..."

(
  echo "Subject: $email_subject"
  echo "To: $recipient_email"
  echo "From: $sender_email"
  echo "MIME-Version: 1.0"
  echo "Content-Type: text/plain"
  
  -----------------------------------------------------------------------------------------
  #!/bin/bash

# List of API endpoints to check
endpoints=(
    "https://api.example.com/endpoint1"
    "https://api.example.com/endpoint2"
    "https://api.example.com/endpoint3"
)

# Create the report file
report_file="api_health_report.txt"

# Function to display a table row
print_table_row() {
    printf "| %-60s | %-20s | %-13s |\n" "$1" "$2" "$3"
}

# Function to display a table separator
print_table_separator() {
    printf "+%s+%s+%s+\n" "------------------------------------------------------------" "----------------------" "---------------"
}

# Generate the report
echo "API Health Check Report" > "$report_file"
echo "" >> "$report_file"
print_table_separator >> "$report_file"
print_table_row "Endpoint" "Status" "Response Code" >> "$report_file"
print_table_separator >> "$report_file"

for endpoint in "${endpoints[@]}"; do
    echo "Checking endpoint: $endpoint"
    
    # Send a GET request to the endpoint and capture the response
    response=$(curl -s -o /dev/null -w "%{http_code}" "$endpoint")

    # Check the HTTP status code
    if [[ "$response" -eq 200 ]]; then
        print_table_row "$endpoint" "Running fine" "$response" >> "$report_file"
    else
        print_table_row "$endpoint" "Not running" "$response" >> "$report_file"
    fi

    print_table_separator >> "$report_file"
done

# Email sending configuration
email_subject="API Health Check Report"
recipient_email="example@example.com"
sender_email="sender@example.com"
smtp_server="smtp.example.com"
smtp_port="587"
smtp_username="your_username"
smtp_password="your_password"

# Send the email with the report file as an attachment
echo "Sending email..."

(
  echo "Subject: $email_subject"
  echo "To: $recipient_email"
  echo "From: $sender_email"
  echo "MIME-Version: 1.0"
  echo "Content-Type: text/plain"
  echo "Content-Disposition: attachment; filename=\"$report_file\""
  cat "$report_file"
) | sendmail -t -oi -f "$sender_email" -S smtp="$smtp_server:$smtp_port" -S smtp-use-starttls -S smtp-auth=login -S smtp-auth-user="$smtp_username" -S smtp-auth-password="$smtp_password" -S ssl-verify=ignore

echo "Email sent to $recipient_email"

# Remove the report file
rm "$report_file"
-------------------------------------------------------------------
  
  
  echo "Content-Disposition: attachment; filename=\"$report_file\""
  cat "$report_file"
) | sendmail -t -oi -f "$sender_email" -S smtp="$smtp_server:$smtp_port" -S smtp-use-starttls -S smtp-auth=login -S smtp-auth-user="$smtp_username" -S smtp-auth-password="$smtp_password" -S ssl-verify=ignore

echo "Email sent to $recipient_email"

# Remove the report file
rm "$report_file"
-------------------------------------------------------------
#!/bin/bash

# Variables
HIVE_SOURCE_QUERY="SELECT * FROM source_table;"
HIVE_TARGET_QUERY="SELECT * FROM target_table;"
BEELINE_CONNECTION_URL="jdbc:hive2://localhost:10000/default"

# Run Hive source query
SOURCE_RESULT=$(beeline -u "$BEELINE_CONNECTION_URL" -e "$HIVE_SOURCE_QUERY" --outputformat=tsv2 --showHeader=false)

# Run Hive target query
TARGET_RESULT=$(beeline -u "$BEELINE_CONNECTION_URL" -e "$HIVE_TARGET_QUERY" --outputformat=tsv2 --showHeader=false)

# Compare results
IFS=$'\n' read -r -a SOURCE_LINES <<< "$SOURCE_RESULT"
IFS=$'\n' read -r -a TARGET_LINES <<< "$TARGET_RESULT"

MISMATCHED_LINES=()
for ((i = 0; i < ${#SOURCE_LINES[@]}; i++)); do
    if [[ "${SOURCE_LINES[$i]}" != "${TARGET_LINES[$i]}" ]]; then
        MISMATCHED_LINES+=($i)
    fi
done

if [ ${#MISMATCHED_LINES[@]} -eq 0 ]; then
    RESULT_STATUS="Success"
else
    RESULT_STATUS="Failure"
fi

# Create HTML report
REPORT_FILE="comparison_report.html"

cat >"$REPORT_FILE" <<EOF
<!DOCTYPE html>
<html>
<head>
<title>Hive Query Comparison Report</title>
</head>
<body>
<h1>Hive Query Comparison Report</h1>
<h2>Source Query:</h2>
<pre>$HIVE_SOURCE_QUERY</pre>
<h2>Target Query:</h2>
<pre>$HIVE_TARGET_QUERY</pre>
<h2>Comparison Result:</h2>
<p>Result: $RESULT_STATUS</p>
<h2>Source Result:</h2>
<pre>$SOURCE_RESULT</pre>
<h2>Target Result:</h2>
<pre>$TARGET_RESULT</pre>
EOF

if [ ${#MISMATCHED_LINES[@]} -gt 0 ]; then
    echo "<h2>Mismatched Lines:</h2>" >>"$REPORT_FILE"
    echo "<ul>" >>"$REPORT_FILE"
    for line_num in "${MISMATCHED_LINES[@]}"; do
        echo "<li>Line $((line_num + 1))</li>" >>"$REPORT_FILE"
    done
    echo "</ul>" >>"$REPORT_FILE"
fi

echo "</body>" >>"$REPORT_FILE"
echo "</html>" >>"$REPORT_FILE"

echo "Comparison report generated: $REPORT_FILE"
-------------------------------------------------------
#!/bin/bash

# Variables
HIVE_SOURCE_QUERY="SELECT * FROM source_table;"
HIVE_TARGET_QUERY="SELECT * FROM target_table;"
BEELINE_CONNECTION_URL="jdbc:hive2://localhost:10000/default"

# Run Hive source query
SOURCE_RESULT=$(beeline -u "$BEELINE_CONNECTION_URL" -e "$HIVE_SOURCE_QUERY" --outputformat=tsv2 --showHeader=false)

# Run Hive target query
TARGET_RESULT=$(beeline -u "$BEELINE_CONNECTION_URL" -e "$HIVE_TARGET_QUERY" --outputformat=tsv2 --showHeader=false)

# Compare results
IFS=$'\n' read -r -a SOURCE_LINES <<< "$SOURCE_RESULT"
IFS=$'\n' read -r -a TARGET_LINES <<< "$TARGET_RESULT"

MISMATCHED_LINES=()
for ((i = 0; i < ${#SOURCE_LINES[@]}; i++)); do
    if [[ "${SOURCE_LINES[$i]}" != "${TARGET_LINES[$i]}" ]]; then
        MISMATCHED_LINES+=("$i")
    fi
done

if [ ${#MISMATCHED_LINES[@]} -eq 0 ]; then
    RESULT_STATUS="Success"
else
    RESULT_STATUS="Failure"
fi

# Create HTML report
REPORT_FILE="comparison_report.html"

cat >"$REPORT_FILE" <<EOF
<!DOCTYPE html>
<html>
<head>
<title>Hive Query Comparison Report</title>
<style>
.diff {
    color: red;
}
</style>
</head>
<body>
<h1>Hive Query Comparison Report</h1>
<h2>Source Query:</h2>
<pre>$HIVE_SOURCE_QUERY</pre>
<h2>Target Query:</h2>
<pre>$HIVE_TARGET_QUERY</pre>
<h2>Comparison Result:</h2>
<p>Result: $RESULT_STATUS</p>
<h2>Source Result:</h2>
<pre>$SOURCE_RESULT</pre>
<h2>Target Result:</h2>
<pre>$TARGET_RESULT</pre>
EOF

if [ ${#MISMATCHED_LINES[@]} -gt 0 ]; then
    echo "<h2>Mismatched Lines:</h2>" >>"$REPORT_FILE"
    for line_num in "${MISMATCHED_LINES[@]}"; do
        echo "<h3>Line $((line_num + 1)):</h3>" >>"$REPORT_FILE"
        echo "<pre>" >>"$REPORT_FILE"
        diff -u <(echo "${SOURCE_LINES[$line_num]}") <(echo "${TARGET_LINES[$line_num]}") | tail -n +4 >>"$REPORT_FILE"
        echo "</pre>" >>"$REPORT_FILE"
    done
fi

echo "</body>" >>"$REPORT_FILE"
echo "</html>" >>"$REPORT_FILE"

echo "Comparison report generated: $REPORT_FILE"
-------------------------------------------------------------------------
#!/bin/bash

# Variables
HIVE_SOURCE_QUERY="SELECT * FROM source_table;"
HIVE_TARGET_QUERY="SELECT * FROM target_table;"
BEELINE_CONNECTION_URL="jdbc:hive2://localhost:10000/default"

# Run Hive source query
SOURCE_RESULT=$(beeline -u "$BEELINE_CONNECTION_URL" -e "$HIVE_SOURCE_QUERY" --outputformat=tsv2 --showHeader=false)

# Run Hive target query
TARGET_RESULT=$(beeline -u "$BEELINE_CONNECTION_URL" -e "$HIVE_TARGET_QUERY" --outputformat=tsv2 --showHeader=false)

# Split results into arrays
IFS=$'\n' read -d '' -r -a SOURCE_LINES <<< "$SOURCE_RESULT"
IFS=$'\n' read -d '' -r -a TARGET_LINES <<< "$TARGET_RESULT"

# Get the number of lines in each result
SOURCE_COUNT=${#SOURCE_LINES[@]}
TARGET_COUNT=${#TARGET_LINES[@]}

# Compare results
if [ "$SOURCE_RESULT" == "$TARGET_RESULT" ]; then
    RESULT_STATUS="Success"
elif [ "$SOURCE_COUNT" -ne "$TARGET_COUNT" ]; then
    RESULT_STATUS="Failure: Mismatched number of records (Source: $SOURCE_COUNT, Target: $TARGET_COUNT)"
else
    MISMATCHED_LINES=()
    for ((i = 0; i < SOURCE_COUNT; i++)); do
        if [[ "${SOURCE_LINES[$i]}" != "${TARGET_LINES[$i]}" ]]; then
            MISMATCHED_LINES+=($i)
        fi
    done

    if [ ${#MISMATCHED_LINES[@]} -eq 0 ]; then
        RESULT_STATUS="Success"
    else
        RESULT_STATUS="Failure"
    fi
fi

# Create HTML report
REPORT_FILE="comparison_report.html"

cat > "$REPORT_FILE" <<EOF
<!DOCTYPE html>
<html>
<head>
<title>Hive Query Comparison Report</title>
<style>
.diff {
    color: red;
}
</style>
</head>
<body>
<h1>Hive Query Comparison Report</h1>
<h2>Source Query:</h2>
<pre>$HIVE_SOURCE_QUERY</pre>
<h2>Target Query:</h2>
<pre>$HIVE_TARGET_QUERY</pre>
<h2>Comparison Result:</h2>
<p>Result: $RESULT_STATUS</p>
<h2>Source Result:</h2>
<pre>$SOURCE_RESULT</pre>
<h2>Target Result:</h2>
<pre>$TARGET_RESULT</pre>
EOF

if [ ${#MISMATCHED_LINES[@]} -gt 0 ]; then
    echo "<h2>Mismatched Lines:</h2>" >> "$REPORT_FILE"
    for line_num in "${MISMATCHED_LINES[@]}"; do
        echo "<h3>Line $((line_num + 1)):</h3>" >> "$REPORT_FILE"
        echo "<pre>" >> "$REPORT_FILE"
        echo "${SOURCE_LINES[$line_num]}" >> "$REPORT_FILE"
        echo "<span class='diff'>" >> "$REPORT_FILE"
        echo "${TARGET_LINES[$line_num]}" >> "$REPORT_FILE"
        echo "</span>" >> "$REPORT_FILE"
        echo "</pre>" >> "$REPORT_FILE"
    done
fi

if [ "$SOURCE_COUNT" -lt "$TARGET_COUNT" ]; then
    echo "<h2>Lines present in Target but not in Source:</h2>" >> "$REPORT_FILE"
    echo "<pre>" >> "$REPORT_FILE"
    for ((i = SOURCE_COUNT; i < TARGET_COUNT; i++)); do
        echo "${TARGET_LINES[$i]}" >> "$REPORT_FILE"
    done
    echo "</pre>" >> "$REPORT_FILE"
fi

echo "</body>" >> "$REPORT_FILE"
echo "</html>" >> "$REPORT_FILE"

echo "Comparison report generated: $REPORT_FILE"
--------------------------------------------------------------------------------------------------
        # Print the merged output file with color highlighting for differences
        awk -F ',' 'NR>1{print "<tr>"; for(i=1;i<=NF;i++){if($i != $(i+NF)){print "<td><font color=\"red\">"$i"</font></td>"}else{print "<td>"$i"</td>"}} print "</tr>"}' "$output_data"
        ------------------------------------------------------------------------------------------------------
- Create table with an array of structs
CREATE TABLE my_table (
  id INT,
  persons ARRAY<STRUCT<name:STRING, age:INT>>
);

-- Insert data into the table
INSERT INTO my_table VALUES
  (1, array(named_struct('name', 'John', 'age', 25), named_struct('name', 'Alice', 'age', 30))),
  (2, array(named_struct('name', 'Bob', 'age', 28), named_struct('name', 'Jane', 'age', NULL)));

- Update age value within the array of structs
UPDATE my_table
SET persons = transform(persons, p -> IF(p.name = 'Jane', named_struct('name', p.name, 'age', 35), p))
WHERE array_contains(transform(persons, p -> p.name), 'Jane');

-- Query the updated table
SELECT * FROM my_table;






Request to Add Data


User
Create a mail saying thanks for adding data in three customers and requesting to add same data in fourth table also in polite manner.. 

ChatGPT
Subject: Gratitude for Adding Data in Three Customers and Kindly Requesting Data Addition to Fourth Table

Dear [Recipient's Name],

I hope this email finds you well. I am writing to express my sincerest gratitude for your recent efforts in adding data to the three customer tables. Your dedication and attention to detail have been invaluable in keeping our records up-to-date and maintaining the efficiency of our systems.

I am delighted to see the progress we've made with the three customer tables, and it has positively impacted our operations. Your commitment to accuracy and timeliness is truly commendable, and we are fortunate to have you as a valued member of our team.

With that said, I kindly request your assistance in adding the same data to the fourth customer table. Your expertise and familiarity with the process make you the perfect person to undertake this task. By extending your efforts to the fourth table, we can ensure that all our customer data is uniform and readily accessible across all records.

I understand that you have a busy schedule, and I genuinely appreciate your willingness to assist with this request. If you encounter any challenges or require any support, please do not hesitate to reach out to me or the relevant team members. We are here to help and support you in any way possible.

Once again, thank you for your exceptional work and your willingness to help with this additional task. Your dedication and teamwork make a significant difference, and we look forward to continuing our collaborative efforts.

Best regards,

[Your Name]
[Your Title/Position]
[Your Company/Organization]
[Your Email Address]
[Your Phone Number]


------------------+------------++-+
#!/bin/bash

if [ "$#" -ne 2 ]; then
  echo "Usage: $0 <tablename> <backuptablename>"
  exit 1
fi

tablename=$1
backuptablename=$2

# Check if the backup table already exists in Hive
backup_table_check=$(hive -S -e "SHOW TABLES LIKE '$backuptablename';")

if [ -n "$backup_table_check" ]; then
  echo "Backup table '$backuptablename' already exists in Hive."
  exit 1
fi

# Run the SHOW CREATE TABLE query and store the output
hive_output=$(hive -S -e "SHOW CREATE TABLE $tablename;")

# Check if the table exists in Hive
if [ -z "$hive_output" ]; then
  echo "Table '$tablename' not found in Hive."
  exit 1
fi

# Replace the tablename with backuptablename in the query
updated_query=$(echo "$hive_output" | sed "s/$tablename/$backuptablename/g")

# Create the backup table in Hive
hive -e "$updated_query"

if [ $? -eq 0 ]; then
  echo "Backup table '$backuptablename' created successfully."
else
  echo "Failed to create backup table '$backuptablename'."
fi

--------------------
cat ~/.ssh/id_rsa.pub | ssh username@IP address "mkdir -p ~/.ssh && chmod 700 ~/.ssh && cat >> ~/.ssh/authorized_keys"


----++-++++++++++++-++++++++
ssh://name@server.com
sftp://name@server.com
ssh -N -L 1337:localhost:1337 name@server.com
ssh -L [LOCAL_IP:]LOCAL_PORT:DESTINATION:DESTINATION_PORT [USER@]SSH_SERVER

---------------------------
{expand:title=Day 1}
  {expand-section:title=Problem}
    Insert problem description for Day 1.
  {expand-section}
  {expand-section:title=Solution}
    Insert code solution for Day 1.
  {expand-section}
{expand}




double compressing files (compressing multiple tar.gz files into one tar.gz file) can potentially cause corruption issues. Compressing an already compressed file may lead to unexpected results, as the compression algorithms might not work optimally on pre-compressed data.

Each compression step introduces additional complexity, and it can be difficult for compression algorithms to efficiently handle data that is already compressed. This can result in higher chances of data corruption, loss of data, or inflated file sizes.

To avoid such issues, it's generally recommended not to compress already compressed files unless you have specific reasons to do so and are aware of the potential risks. If you need to distribute multiple tar.gz files together, it's better to keep them separate and provide clear instructions for decompressing and using them individually.

--------------------------------------


#!/bin/bash

# Check if the correct number of arguments is provided
if [ "$#" -ne 3 ]; then
    echo "Usage: $0 <source_database> <source_table> <backup_table>"
    exit 1
fi

# Store the arguments in variables
source_database="$1"
source_table="$2"
backup_table="$3"

# Run the Hive query to get the CREATE TABLE statement
create_statement=$(hive -S -e "SHOW CREATE TABLE $source_database.$source_table")

# Check if the query executed successfully
if [ $? -eq 0 ]; then
    # Modify the CREATE TABLE statement for the backup
    modified_statement=$(echo "$create_statement" | sed "s/$source_table/$backup_table/g")

    # Create the backup table using the modified statement
    hive -e "$modified_statement"
    echo "Backup table '$backup_table' created successfully!"
else
    echo "Error running Hive query. Check if the source database and table exist."
    exit 1
fi




#!/bin/bash

# Check if the table names file exists
table_names_file="table_names.txt"

if [ ! -f "$table_names_file" ]; then
    echo "Table names file '$table_names_file' not found."
    exit 1
fi

# Read table names from the text file into an array
readarray -t table_names < "$table_names_file"

# Loop through each table
for table_name in "${table_names[@]}"; do
    # Define the output SQL file name for this table
    output_file="${table_name}_insert_queries.sql"

    # Clear the contents of the output file (if it exists)
    > "$output_file"

    # Loop to generate INSERT queries with sys_recordkey values for this table
    for ((i = 3000; i <= 8000; i++)); do
        sys_recordkey="RecordKey_$i"

        # Create the INSERT query
        insert_query="INSERT INTO $table_name (sys_recordkey) VALUES ('$sys_recordkey');"

        # Append the INSERT query to the output file
        echo "$insert_query" >> "$output_file"
    done

    # Add the 'run_sql' line to the output file
    echo "m_db run dbc -file $output_file" >> "$output_file"

    echo "SQL insert queries for $table_name generated and saved in $output_file"
done




from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import time

driver = webdriver.Chrome()
driver.get("https://google.com")

---------------------------------
# Check if the first value of policycodes_list is in the associative array
        if [ -n "${array3[${policycodes_list[0]}]}" ]; then
            policydecision_list+=("${array3[${policycodes_list[0]}]}")
        else
            policydecision_list+=("V99")
        fi
array3=( ["PC002"]="V1" ["PC003"]="V2" ["PC001"]="V3" )


--------------------------------------

#!/bin/bash

csv_file="/Inbound/exclusion.csv"

# Define the associative array with key-value pairs
declare -A array3
array3=( ["PC002"]="V1" ["PC003"]="V2" ["PC001"]="V3" )

echo $csv_file

while IFS='|' read -r customerid policycodes previousadverseactioncode; do
    # Remove square brackets and split policycodes into an array
    policycodes_list=($(sed 's/\[//;s/\]//;s/,/\n/g;s/\"//g' <<< "$policycodes"))

    # Remove square brackets and split previousadverseactioncode into an array
    previousadverseactioncode_list=($(sed 's/\[//;s/\]//;s/,/\n/g;s/\"//g' <<< "$previousadverseactioncode"))

    echo "Customer ID: $customerid"
    policydecision_list=()  # New list to hold policy decision values
    priority_list=()  # New list to hold priority values

    # Check conditions for priority_list
    if [ "${previousadverseactioncode_list[0]}" == "NULL" ] || [ "${previousadverseactioncode_list[3]}" == "NULL" ] || [ "${previousadverseactioncode_list[4]}" == "NULL" ] || [ "${previousadverseactioncode_list[5]}" == "NULL" ] || [ "${previousadverseactioncode_list[6]}" == "NULL" ]; then
        priority_list+=(-999 -999 -999 -999)
    elif [ ${#policycodes_list[@]} -eq 0 ] && [ "${previousadverseactioncode_list[0]}" == "null" ] && [ "${previousadverseactioncode_list[1]}" == "null" ] && [ "${previousadverseactioncode_list[2]}" == "null" ] && [ "${previousadverseactioncode_list[3]}" == "null" ]; then
        priority_list+=(-998 -998 -998 -998)
    else
        # Add padding of 0s if policycodes_list has less than four values
        while [ ${#policycodes_list[@]} -lt 4 ]; do
            policycodes_list+=("0")
        done

        if [ "${previousadverseactioncode_list[@]:0:4}" == "${policycodes_list[@]:0:4}" ]; then
            priority_list+=(-997 -997 -997 -997)
        else
            priority_list+=("Unknown" "Unknown" "Unknown" "Unknown")
        fi
    fi

    echo "Priority List: ${priority_list[@]}"
done < "$csv_file"


SELECT id, name, address, phone
FROM customer
LATERAL VIEW explode(array_repeat(struct(*), 6)) exploded_table AS exploded_row;


SELECT id, name, address, phone, col
FROM customer
LATERAL VIEW explode(array('A', 'B', 'C', 'D', 'E', 'F')) exploded_table AS col;

--------------------------------------------------------------------------------


Day 1: Introduction to Linux Commands
Session 1 (30 minutes)

Introduction to Linux:

Provide a brief overview of Linux and its significance.
Explain the concept of a shell and the Bash shell.
Command Line Interface (CLI) Basics:

Discuss the basic structure of a Linux command: command, options, and arguments.
Demonstrate how to access the terminal in Linux.
Session 2 (30 minutes)

Navigating the File System:

Cover essential commands: pwd, ls, cd, mkdir, rmdir.
Explain relative and absolute paths.
File and Directory Manipulation:

Discuss commands: cp, mv, rm.
Introduce wildcard characters (*, ?).
Day 2: Advanced Linux Commands and Text Processing
Session 1 (30 minutes)

File and Text Processing with Awk and Sed:
Introduce awk for text and data extraction, covering basic syntax and examples.
Explain sed for text stream editing, focusing on search and replace operations.
Session 2 (30 minutes)

Advanced Text Processing with Awk and Sed:
Demonstrate advanced text filtering, transformation, and substitution using awk and sed.
Provide practical examples and exercises for students to practice awk and sed commands.
Day 3: Introduction to Shell Scripting
Session 1 (30 minutes)

Basics of Shell Scripting:

Define a shell script and explain its purpose.
Guide students in creating a simple shell script, explaining the script's structure.
Variables and Control Structures:

Introduce variables, data types, and their usage in shell scripts.
Cover control structures: if statements for conditional branching and loops (for and while loops).
Session 2 (30 minutes)

Functions and Script Organization:
Describe functions in shell scripting, including declaration and usage.
Emphasize organizing scripts into functions for modularity and readability.
----------------------------------------------------------------------------------

1/ Question: What is the purpose of the shebang
(#!) at the beginning of a shell script? Give an
example.
Answer: The shebang specifies the interpreter for the
script.
Example: #!/bin/bash indicates the script is using the
Bash shell.
---------------------------------------------------------
2/ Question: How can you pass arguments to a
shell script? Provide an example.
Answer: Use $1, $2, etc., for positional arguments.
Example: ./myscript.sh arg1 arg2.
---------------------------------------------------------
3/ Question: Explain the difference between single
and double quotes in shell scripting.
Answer: Single quotes preserve the literal value, while
double quotes allow variable expansion.
Example: echo 'Hello $USER' vs. echo "Hello
$USER".
---------------------------------------------------------
4/ Question: What is command substitution, and
how can you use it in a script? Provide an example.
Answer: Command substitution allows embedding
command output.
Example: current_date=$(date).
---------------------------------------------------------
5/ Question: How do you check if a file exists in a
shell script? Write a conditional example.
Answer: Use if [ -e "$filename" ]; then ... fi.
---------------------------------------------------------
6/ Question: Explain what the $? variable
represents in shell scripting.
Answer: $? holds the exit status of the last command.
0 means success, non-zero indicates an error.
---------------------------------------------------------
7/ Question: What is a here document in shell
scripting, and when would you use it? Show an
example.
Answer: A here document is used for multiline input.
Example:
cat <<EOF
This is 
a multiline
text.
EOF
---------------------------------------------------------
8/ Question: How can you iterate over files in a
directory using a for loop in Bash? Provide an
example.
Answer: Example:
for file in/path/to/dir/*; do 
	echo "$file"
done
---------------------------------------------------------
9/ Question: What is process substitution in Bash?
Give an example.
Answer: Process substitution allows treating the
output of a command as a file.
Example: diff <(command1) <(command2).
---------------------------------------------------------
10/ Question: How can you redirect both stdout and
stderr to a file in a single command?
Answer: Use command &> output.log.
---------------------------------------------------------
11/ Question: Explain the purpose of the set -e
option in a shell script.
Answer: set -e makes the script exit if any command
returns a non-zero status.
---------------------------------------------------------
12/ Question: What is a pipeline in shell scripting,
and why is it useful? Provide an example.
Answer: A pipeline connects the stdout of one
command to the stdin of another.
Example: cat file.txt | grep "pattern".
---------------------------------------------------------
13/ Question: How can you create and use
functions in a shell script? Show a function definition
and usage.
Answer: Example:
my_function() {
	echo "Hello, $1!"
}
my_function "Prasad"
---------------------------------------------------------
14/ Question: What is process forking in shell
scripting? Give an example.
Answer: Process forking is creating new processes.
Example: forked_process &.
---------------------------------------------------------
15/ Question: Explain the difference between ==
and = in a shell script's conditional statements.
Answer: == is used for string comparisons, while = is
used for variable assignments.
---------------------------------------------------------
16/ Question: How can you trap signals in a shell
script? Provide an example.
Answer: Example to trap SIGINT (Ctrl+C):
trap 'echo "Cntrl+C pressed"' INT
---------------------------------------------------------
17/ Question: What are associative arrays in Bash,
and how do you use them? Show an example.
Answer: Associative arrays allow key-value pairs.
Example:
declare -A fruits
fruits["apple]="red"
echo "Color of apple: ${fruits["apple"]}
---------------------------------------------------------
18/ Question: How can you check if a variable is
empty in a shell script?
Answer: Use if [ -z "$var" ]; then ... fi.
19/ Question: What is the purpose of the read
command in shell scripting? Provide an example.
Answer: read is used to read user input.
Example:
echo "Enter your name:"
read name 
echo "Hello, $name!"
---------------------------------------------------------
20/ Question: Explain the difference between &&
and || in conditional statements.
Answer: && executes the right command if the left
succeeds. || executes the right command if the left
fails.
---------------------------------------------------------
Time for Bonus
1/ Advanced Q8: How can you create a shell script
that runs as a daemon or background process?
A1: Use '&' to run in the background and 'nohup' to
detach from the terminal.
Example:
nohup ./myscript.sh &
---------------------------------------------------------
2/ Advanced Q5: Explain the use of 'select' in shell
scripting.
A2: 'select' provides a menu-driven interface for users.
Example:
PS3="Choose an option: "
options=("Option 1" "Option 2" "Quit")
select choice in "$(option[@]}"; do
	case $choice in 
	"Option 1")
	echo "you chose Option 1"
        ;;
	"Option 2")
	echo "you chose Option 1"
	;;
	"Quit")
	break
	;;
	*)
	echo "Invalid option"
	;;
	esac
done
---------------------------------------------------------
3/ Advanced Q4: What is process control in shell
scripting, and how can you use it?
A3: Process control involves managing background
jobs.
Example:
command1 &
command2 &
wait
---------------------------------------------------------
4/ Advanced Q4: How do you create and
manipulate associative arrays in Bash?
A4: Declare an associative array and use it like this:
declare -A fruits
fruits["apple]="red"
echo "Color of apple: ${fruits["apple"]}
---------------------------------------------------------
5/ : Explain 'awk' one-liners in shell scripting and
provide an example.
A5: 'awk' one-liners are concise commands for text
processing. Example:
awk '/pattern/' {print $2}
---------------------------------------------------------

Certainly! Here are examples for each of the shell redirection and pipe operations:

1. `>` (Greater Than) Example:
   - Redirecting the output of a command to a file.
   ```bash
   echo "Hello, World!" > output.txt
   ```
   This will create a file named `output.txt` containing the text "Hello, World!".

2. `>>` (Double Greater Than) Example:
   - Appending the output of a command to an existing file.
   ```bash
   echo "This is the second line." >> output.txt
   ```
   This will append the text "This is the second line." to the `output.txt` file created in the previous example.

3. `<` (Less Than) Example:
   - Redirecting input to a command from a file.
   ```bash
   cat < input.txt
   ```
   This will display the contents of the `input.txt` file using the `cat` command.

4. `|` (Pipe) Example:
   - Using a pipe to pass output from one command as input to another.
   ```bash
   ls | grep "example"
   ```
   This will list files in the current directory using `ls` and then filter the output using `grep` to display only the lines containing the word "example".

5. `tee` Example:
   - Using `tee` to capture output in a file and display it simultaneously.
   ```bash
   echo "This is some data." | tee output.txt
   ```
   This will display "This is some data." in the terminal and also save it to the `output.txt` file.

These examples demonstrate how each of these shell operations can be used to manipulate input and output in various ways within a shell script.


------------------------------------------------------------------------------------------

Certainly! Let's explore `sed`, `awk`, and `grep` commands in detail with examples and cover some of their common arguments.

### `sed` Command:
`sed` stands for stream editor and is used for text manipulation. It operates on a line-by-line basis and can perform tasks like search, replace, insert, and delete text.

**Basic Syntax:**
```bash
sed 's/pattern/replacement/g' filename
```

- `pattern`: The text you want to replace.
- `replacement`: The text you want to replace `pattern` with.
- `g`: Global flag, replaces all occurrences of `pattern` (not just the first one).

**Example:**
```bash
echo "Hello, World!" | sed 's/World/Universe/g'
```
This will output: `Hello, Universe!`

### `awk` Command:
`awk` is a powerful programming language mainly used for pattern scanning and processing. It operates on a per-line basis and is excellent for processing structured text data.

**Basic Syntax:**
```bash
awk '{pattern_action}' filename
```

- `{pattern_action}`: Defines patterns to match and actions to perform. Patterns are enclosed in slashes (e.g., `/pattern/`) and associated actions are inside curly braces.

**Example:**
```bash
echo "John 25\nAlice 30\nBob 28" | awk '{print $1, "is", $2, "years old."}'
```
This will output:
```
John is 25 years old.
Alice is 30 years old.
Bob is 28 years old.
```

### `grep` Command:
`grep` is used for searching text using patterns. It can search for patterns in files or output generated by other commands.

**Basic Syntax:**
```bash
grep 'pattern' filename
```

- `pattern`: The text pattern you want to search for in the file.

**Example:**
```bash
echo -e "apple\nbanana\norange\ngrape" | grep 'an'
```
This will output:
```
banana
orange
```

#### Common `grep` Arguments:
- `-i`: Ignore case (case-insensitive search).
- `-r` or `-R`: Recursive search in directories.
- `-n`: Show line numbers along with matching lines.
- `-v`: Invert match, i.e., show non-matching lines.
- `-e pattern`: Specify multiple patterns to search for.
- `-l`: Show only the names of files with matching lines.
- `-c`: Show only the count of matching lines.

**Example using arguments:**
```bash
grep -rin 'pattern' directory/
```
This will perform a case-insensitive, recursive search for 'pattern' in files inside the specified directory, showing line numbers for matches.

These examples cover the basic usage and some common arguments for `sed`, `awk`, and `grep`. Each of these commands has many more advanced features and options, so feel free to explore their respective documentation for further details.
------------------------------------------------------------
Certainly! In shell scripting, loops are used to execute a block of code repeatedly based on a condition. There are mainly three types of loops: `for`, `while`, and `until`. Additionally, `IFS` (Internal Field Separator) is not a loop but an environment variable used to split a string into fields based on a delimiter. Let's explore each one:

### 1. `for` Loop:
The `for` loop is used to iterate a specific set of statements for a predefined number of times or over a list of items.

**Syntax:**
```bash
for variable in list
do
    # Code to be executed for each item in the list
done
```

**Example:**
```bash
for i in {1..5}
do
    echo "Number: $i"
done
```
This will print numbers from 1 to 5.

### 2. `while` Loop:
The `while` loop is used to execute a set of commands as long as a specified condition is true.

**Syntax:**
```bash
while [ condition ]
do
    # Code to be executed as long as the condition is true
done
```

**Example:**
```bash
count=1
while [ $count -le 5 ]
do
    echo "Count: $count"
    ((count++))
done
```
This will print numbers from 1 to 5 using a `while` loop.

### 3. `until` Loop:
The `until` loop is used to execute a set of commands as long as a specified condition is false.

**Syntax:**
```bash
until [ condition ]
do
    # Code to be executed as long as the condition is false
done
```

**Example:**
```bash
count=1
until [ $count -gt 5 ]
do
    echo "Count: $count"
    ((count++))
done
```
This will print numbers from 1 to 5 using an `until` loop.

### `IFS` (Internal Field Separator):
`IFS` is not a loop, but an environment variable used to split a string into fields based on a delimiter.

**Example:**
```bash
sentence="This is a sentence"
IFS=" " # Setting space as the delimiter
read -ra words <<< "$sentence"
for word in "${words[@]}"
do
    echo "Word: $word"
done
```
This will split the sentence into words based on spaces and print each word separately.

These examples cover different types of loops in shell scripting (`for`, `while`, and `until`) as well as the usage of `IFS` to split strings. Each type of loop has its own specific use cases, allowing you to create flexible and powerful scripts.
----------------------------------------------------
Command substitution in shell scripting allows you to execute a command and use its output (stdout) as a value in the script. This can be achieved using backticks (\``) or `$()` syntax. Here's how it works:

### Using Backticks (\``):
```bash
variable_name=`command`
```

### Using `$()` Syntax:
```bash
variable_name=$(command)
```

Here are a few examples to illustrate command substitution:

**Example 1: Capturing Command Output**
```bash
current_date=`date`
echo "Current date and time: $current_date"
```
In this example, the `date` command is executed, and its output is stored in the `current_date` variable, which is then printed.

**Example 2: Command Substitution with a File**
```bash
file_count=$(ls -l | wc -l)
echo "Number of files in the current directory: $file_count"
```
In this example, `ls -l` lists the files in the current directory, `wc -l` counts the number of lines in the output, and the result is stored in the `file_count` variable.

**Example 3: Using Command Substitution in a Command**
```bash
echo "Today is $(date +%A), $(date +%B) $(date +%d), $(date +%Y)"
```
In this example, the `date` command is used within the `echo` statement using command substitution to print the current day, month, date, and year.

**Example 4: Command Substitution with Arithmetic**
```bash
result=$(( $(echo "10 + 20") ))
echo "Result of the arithmetic operation: $result"
```
In this example, command substitution is used inside an arithmetic operation to calculate the sum of 10 and 20.

Command substitution is a powerful feature that allows you to capture the output of commands and use it as variables or inputs for other commands within your shell scripts.
--------------------------------------------------------------------------------
In shell scripting, `if-else` statements are used to make decisions based on conditions. Here's the basic structure:

```bash
if [ condition ]
then
    # Code to be executed if the condition is true
else
    # Code to be executed if the condition is false
fi
```

The square brackets `[ ]` are used for evaluating the condition. The `then` keyword indicates the start of the code block to be executed if the condition is true. The `else` keyword is followed by the code block to be executed if the condition is false. The `fi` keyword marks the end of the `if-else` block.

Here are a few examples to illustrate `if-else` statements in shell scripting:

**Example 1: Checking if a Number is Even or Odd**
```bash
echo "Enter a number: "
read number

if [ $((number % 2)) -eq 0 ]
then
    echo "$number is even."
else
    echo "$number is odd."
fi
```

In this example, the script checks if the input number is even or odd and prints the corresponding message.

**Example 2: Comparing Strings**
```bash
expected="password123"
echo "Enter the password: "
read password

if [ "$password" = "$expected" ]
then
    echo "Access granted."
else
    echo "Access denied."
fi
```

In this example, the script checks if the entered password matches the expected password and grants or denies access accordingly.

**Example 3: Nested `if-else`**
```bash
echo "Enter your age: "
read age

if [ $age -ge 18 ]
then
    echo "You are eligible to vote."
    
    if [ $age -ge 60 ]
    then
        echo "You are also eligible for senior citizen benefits."
    fi
else
    echo "You are not eligible to vote."
fi
```

In this example, the script checks the age and provides different messages based on whether the person is eligible to vote and whether they qualify for senior citizen benefits.

`if-else` statements allow you to create conditional logic in your shell scripts, making it possible to execute different code blocks based on specific conditions.
----------------------------------------------------------------------------------------------------
In shell scripting, **arguments** refer to the values provided to a script or a function when it is executed. These arguments allow you to pass input data to your script or function, making them dynamic and versatile. 

### Arguments in Shell Script:

**Syntax for accessing script arguments:**
- `$0`: Name of the script itself.
- `$1`, `$2`, `$3`, ...: The first, second, third, and so on, arguments passed to the script.

**Example Script:**
```bash
#!/bin/bash

echo "Script name: $0"
echo "First argument: $1"
echo "Second argument: $2"
echo "All arguments: $@"
echo "Number of arguments: $#"
```

In this script, `$0` represents the script name, `$1` and `$2` represent the first and second arguments passed to the script, respectively. `$@` represents all the arguments, and `$#` represents the total number of arguments passed.

**Executing the Script:**
```bash
./script.sh arg1 arg2
```
Output:
```
Script name: ./script.sh
First argument: arg1
Second argument: arg2
All arguments: arg1 arg2
Number of arguments: 2
```

### Arguments in Functions:

**Syntax for accessing function arguments:**
- `$1`, `$2`, `$3`, ...: The first, second, third, and so on, arguments passed to the function.

**Example Function:**
```bash
#!/bin/bash

function greet {
    echo "Hello, $1!"
}

greet "Alice"
greet "Bob"
```

In this function, `$1` represents the first argument passed to the `greet` function.

**Function Execution:**
```bash
./script.sh
```
Output:
```
Hello, Alice!
Hello, Bob!
```

In the example, the `greet` function is called twice with different arguments.

You can use these arguments to make your scripts and functions more flexible and capable of handling various input scenarios. These arguments allow your scripts and functions to be reused with different data without modifying the script logic.
------------------------------------------------------------------------------
SMTP (Simple Mail Transfer Protocol) is a protocol used in email communication to send messages from a client to a server or between servers. You can use the `telnet` command in a shell script to interact with an SMTP server and send emails using the SMTP protocol. Below is an example of how you can send an email using SMTP in a shell script using the `telnet` command with verbose output.

**Shell Script to Send Email using SMTP:**

```bash
#!/bin/bash

# SMTP Server Details
SMTP_SERVER="smtp.example.com"
SMTP_PORT="25"
SENDER_EMAIL="sender@example.com"
RECIPIENT_EMAIL="recipient@example.com"

# Email Content
EMAIL_SUBJECT="Test Email"
EMAIL_BODY="This is a test email sent using SMTP in shell script."

# Connect to SMTP server using telnet
{
    sleep 1
    echo "EHLO example.com"
    sleep 1
    echo "MAIL FROM:<$SENDER_EMAIL>"
    sleep 1
    echo "RCPT TO:<$RECIPIENT_EMAIL>"
    sleep 1
    echo "DATA"
    sleep 1
    echo "Subject: $EMAIL_SUBJECT"
    echo -e "$EMAIL_BODY"
    echo "."
    echo "QUIT"
} | telnet -c "$SMTP_SERVER" "$SMTP_PORT"

echo "Email sent successfully."
```

In this script:

1. Replace `"smtp.example.com"` with your SMTP server address.
2. Set `"25"` as the port number for unencrypted SMTP communication. For encrypted SMTP (SMTPS) use port `"587"` or `"465"` depending on the encryption method (STARTTLS or SSL/TLS).
3. Set `"sender@example.com"` and `"recipient@example.com"` with the sender and recipient email addresses, respectively.
4. Set the `EMAIL_SUBJECT` and `EMAIL_BODY` variables with the desired email subject and body.

The script uses `telnet` to connect to the SMTP server and sends SMTP commands to send the email. The `-c` option in `telnet` enables character mode, allowing you to send commands non-interactively. The `sleep` commands are added to give the server some time to respond between each command.

Make sure to customize the script with your SMTP server details and run it in a shell environment that allows network connections. Note that this example does not handle authentication or encryption, which are typically required in real-world scenarios.
---------------------------------------------------------
Certainly! The `curl` command in shell scripting is a powerful tool for making HTTP requests to various services, including APIs. Below is an example of how you can use `curl` in a shell script to interact with the Jira API for demonstration purposes.

### Prerequisites:
Before using the Jira API, make sure you have the following information:
- Jira Base URL
- API endpoint you want to access
- Authentication credentials (usually in the form of username and API token)

### Example Shell Script Using `curl` and Jira API:

```bash
#!/bin/bash

# Jira API Details
JIRA_BASE_URL="https://your-jira-instance.atlassian.net"
JIRA_API_ENDPOINT="/rest/api/2/issue"
USERNAME="your_username"
API_TOKEN="your_api_token"

# Issue Key to Fetch (replace 'PROJECT-123' with a valid Jira issue key)
ISSUE_KEY="PROJECT-123"

# API Request URL
API_URL="$JIRA_BASE_URL$JIRA_API_ENDPOINT/$ISSUE_KEY"

# Curl command to fetch Jira issue details
curl -u "$USERNAME:$API_TOKEN" -X GET -H "Content-Type: application/json" "$API_URL"
```

In this script:

- `JIRA_BASE_URL` is the base URL of your Jira instance.
- `JIRA_API_ENDPOINT` is the specific API endpoint you want to access (in this case, fetching a specific issue).
- `USERNAME` is your Jira username.
- `API_TOKEN` is your Jira API token. You can generate an API token in your Jira account settings.
- `ISSUE_KEY` is the key of the Jira issue you want to fetch. Replace it with a valid issue key from your Jira instance.

The `curl` command makes a GET request to the specified API endpoint with basic authentication using your Jira username and API token. The response from the Jira API will be printed to the terminal.

Make sure to update the variables (`JIRA_BASE_URL`, `JIRA_API_ENDPOINT`, `USERNAME`, `API_TOKEN`, and `ISSUE_KEY`) with your own Jira details before running the script. This script demonstrates how to fetch issue details, but you can modify it to perform other operations based on the Jira API documentation.
---------------------------------------------------------------------
Birthday Mail
Certainly! Below is an example of a shell script that reads a CSV file with pipe-delimited fields containing name, email, and birthday information. The script checks if today's date matches any of the birthdays in the file and sends a birthday wish email to the respective person using the SMTP protocol. Please replace `smtp_server`, `smtp_port`, `smtp_username`, `smtp_password` with your SMTP server details and provide the correct path to your CSV file.

```bash
#!/bin/bash

# SMTP Server details
smtp_server="your_smtp_server"
smtp_port="your_smtp_port"
smtp_username="your_smtp_username"
smtp_password="your_smtp_password"

# CSV file path
csv_file="path/to/your/csvfile.csv"

# Get current date in format MM-DD
current_date=$(date +%m-%d)

# Read the CSV file and send birthday wishes
while IFS='|' read -r name email birthday; do
    # Extract month and day from the birthday field (MM-DD)
    birthday_month_day=$(echo "$birthday" | cut -d'-' -f2-)

    # Check if current date matches birthday
    if [ "$current_date" == "$birthday_month_day" ]; then
        # Send birthday wish email
        echo "Subject: Happy Birthday, $name!
From: Your Name <your_email@example.com>
To: $email

Dear $name,

Happy Birthday! üéâüéÇ

Best regards,
Your Name" | \
        /usr/sbin/sendmail -S "$smtp_server:$smtp_port" \
                            -au"$smtp_username" -ap"$smtp_password" \
                            -f"your_email@example.com" "$email"
        
        echo "Birthday wish sent to $name ($email)"
    fi
done < "$csv_file"
```

Make sure to update the SMTP server details, CSV file path, and customize the email message as needed. Please note that sending emails via SMTP may require appropriate permissions and configurations on your server. Also, ensure that the `sendmail` command is available on your system. You might need to adjust the command if you are using a different mail sending method.
---------------------------------------------------------

I have csv pipe delimited file with name|mail-id|birthday. create a shell script which will check the current date if currecnt date matches with the birthday then we will send the birthday wish to the person via mail using smtp protocol



-------------------------------------------------------
<dependency>  
    <groupId>com.sparkjava</groupId>  
    <artifactId>spark-core</artifactId>  
    <version>2.9.3</version>  
</dependency>  
-----------------------------
import static spark.Spark.*;  
public class HelloWorld   
{  
    public static void main(String[] args)   
{  
        get("/hello", (req, res) -> "Hello World");  
    }  
}  

https://www.javatpoint.com/spark-java

SparkSession spark = SparkSession.builder().master("yarn").appName("Application").enableHiveSupport().getOrCreate();
Dataset<Row> ds = spark.read().parquet(filename);

---------------------------------------------------

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class ParquetFileReader {
    public static void main(String[] args) {
        // Create a SparkSession
        SparkSession spark = SparkSession.builder()
                .appName("ParquetFileReader")
                .master("local[*]") // You can change the master URL based on your Spark cluster configuration
                .getOrCreate();

        // Specify the path to the Parquet file
        String parquetFilePath = "path/to/your/parquet/file";

        // Read the Parquet file into a DataFrame
        Dataset<Row> parquetFile = spark.read().parquet(parquetFilePath);

        // Show the schema of the DataFrame
        parquetFile.printSchema();

        // Show the data in the DataFrame
        parquetFile.show();

        // Stop the SparkSession
        spark.stop();
    }
}

-----+----------------+---------------
no give java code

To execute SQL queries on a dataset in Spark using Java, you can follow these steps:

1. Create a `SparkSession` object:

```java
SparkSession spark = SparkSession.builder()
    .appName("Java Spark SQL basic example")
    .config("spark.some.config.option", "some-value")
    .getOrCreate();
```

2. Read the dataset and create a temporary view:

```java
Dataset<Row> df = spark.read().json("path/to/dataset.json");
df.createOrReplaceTempView("people");
```

3. Run an SQL query on the dataset:

```java
Dataset<Row> sqlDF = spark.sql("SELECT * FROM people WHERE age > 30");
sqlDF.show();
```

Here's the complete code:

```java
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class SparkSQLExample {
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
            .appName("Java Spark SQL basic example")
            .config("spark.some.config.option", "some-value")
            .getOrCreate();

        Dataset<Row> df = spark.read().json("path/to/dataset.json");
        df.createOrReplaceTempView("people");

        Dataset<Row> sqlDF = spark.sql("SELECT * FROM people WHERE age > 30");
        sqlDF.show();
    }
}
```

Make sure to replace `"path/to/dataset.json"` with the actual path to your dataset. This code reads a JSON file and performs a simple SQL query to filter the data based on the age column. You can modify the SQL query to suit your needs.
------------------------------------------------------------------

#!/bin/bash

# Function to check the zip file and its contents
checkZipFile() {
    local directory="/path/to/your/directory"
    local log_file="/path/to/your/logfile.log"
    local graph_count=0
    
    

    # Call the function to run .pset files
    runGraphs "$directory" "$log_file" graph_count
    
    # Log the total number of graphs processed along with the directory path
    echo "$(date) - Total graphs processed from $directory : $graph_count" >> "$log_file"
}

# Function to run .pset files
runGraphs() {
    local dir="$1"
    local log_file="$2"
    local -n count_ref="$3"
    
    # Loop through .pset files and run 'air run {path}'
    for pset_file in "$dir"/*.pset; do
        if [ -e "$pset_file" ]; then
            echo "Running air run for file: $pset_file"
            
            # Run the graph and capture the output in a variable
            graph_output=$(air run "$pset_file")
            
            # Check if the last line contains '+++++COMPLETE+++++'
            if [[ "$graph_output" == *'+++++COMPLETE+++++'* ]]; then
                echo "Graph execution successful."
            else
                echo "Graph execution failed."
                # Log the message
                echo "$(date) - Graph execution failed for file: $pset_file" >> "$log_file"
            fi
            
            # Increment the graph count
            ((count_ref++))
        else
            echo "No .pset files found in the directory."
            # Log the message
            echo "$(date) - No .pset files found in the directory." >> "$log_file"
        fi
    done
}

# Call the function to check the zip file and run .pset files
checkZipFile
--------------------------------------------------------------
#!/bin/bash

# Specify the location to check for zip file
file_location="/path/to/your/directory"

# Log function to store messages in a log file
log() {
    local log_file="/path/to/your/logfile.log"
    echo "$(date) - $1" >> "$log_file"
}

# Check if zip file exists
if [ -e "$file_location/abc-$(date +"%d-%m-%Y").zip" ]; then
    echo "Zip file found: abc-$(date +"%d-%m-%Y").zip"
    
    # Extract date from zip file name
    zip_date=$(date -d "$(date +"%d-%m-%Y")" +"%d-%m-%Y")

    # Extract text files from the zip and check their names for the same date
    unzip -l "$file_location/abc-$zip_date.zip" '*.txt' | awk '{print $4}' | while read -r filename; do
        # Extract date from text file name
        text_file_date=$(echo "$filename" | grep -oP '\d{2}-\d{2}-\d{4}')
        if [ "$text_file_date" == "$zip_date" ]; then
            echo "Text file $filename has the correct date."
        else
            echo "Error: Text file $filename does not have the correct date."
            # Log the message
            log "Error: Text file $filename does not have the correct date."
        fi
    done

else
    echo "Zip file not found: abc-$(date +"%d-%m-%Y").zip"
    # Log the message
    log "Zip file not found: abc-$(date +"%d-%m-%Y").zip"
fi
--------------------------------------------------------
#!/bin/bash

# Function to log messages
log() {
    local log_file="/path/to/your/logfile.log"
    echo "$(date) - $1" >> "$log_file"
}

# Function to check the zip file and its contents
checkZipFile() {
    local directory="/path/to/your/directory"
    local graph_count=0
    
    # ... (your existing code for checking zip files)

    # Call the function to run .pset files
    runGraphs "$directory" graph_count
    
    # Log the total number of graphs processed along with the directory path
    log "Total graphs processed from $directory: $graph_count"
}

# Function to run .pset files
runGraphs() {
    local dir="$1"
    local -n count_ref="$2"
    
    # Loop through .pset files and run 'air run {path}'
    for pset_file in "$dir"/*.pset; do
        if [ -e "$pset_file" ]; then
            echo "Running air run for file: $pset_file"
            
            # Run the graph and capture the output in a variable
            graph_output=$(air run "$pset_file")
            
            # Check if the last line contains '+++++COMPLETE+++++'
            if [[ "$graph_output" == *'+++++COMPLETE+++++'* ]]; then
                echo "Graph execution successful."
            else
                echo "Graph execution failed."
                # Log the message
                log "Graph execution failed for file: $pset_file"
            fi
            
            # Increment the graph count
            ((count_ref++))
        else
            echo "No .pset files found in the directory."
            # Log the message
            log "No .pset files found in the directory."
        fi
    done
}

# Call the function to check the zip file and run .pset files
checkZipFile
-----------------------------------------------------
for file in $(unzip -l archive.zip | awk '{print $NF}'); do
  last_line=$(unzip -p archive.zip "$file" | tail -n 1)
  echo "$file: $last_line"
done
---------------------------------------------------------
To create temporary log files in Linux, you can use the mktemp utility available on most Linux systems
This utility creates unique temporary files within the containing directory. By default, running mktemp with no arguments will create a temporary file in /tmp
TMPFILE=$(mktemp /tmp/foo-XXXXX)
echo "Hello world" >> $TMPFILE  # Append some text to the file
cat $TMPFILE                    # Print the contents of the file
rm $TMPFILE         

TMPFILE=$(mktemp /tmp/foo-XXXXXX)
trap "rm -f $TMPFILE" EXIT
When keeping big temporary files, we might want to do some housekeeping in case our script fails.

We can do this using the trap command, which allows us to run some code when an exit code is detected:
$ FILENAME=$(mktemp --tmpdir=/tmp temp_file-XXXXXXXXXX.txt)
$ echo $FILENAME
/tmp/temp_file-HIitKZc0MT.txt
-------------------------------------------------------------
extracted_date=$(echo "$filename" | sed -n 's/.*_\([0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}\)_.*/\1/p')

-------------------------------------------------------------
#!/bin/bash

# Record start time
start_time=$(date "+%Y-%m-%d %H:%M:%S")

# Your existing shell script commands here
# For example:
# command1
# command2
# ...

# Record end time
end_time=$(date "+%Y-%m-%d %H:%M:%S")

# Find files generated between start and end time
find /path/to/your/directory -type f -newermt "$start_time" ! -newermt "$end_time"
------------------------------------------------------------------
#!/bin/bash

# Prompt user for name
read -p "Enter your name: " name

# Prompt user for rating
read -p "Please rate our training out of 5: " rating

# Check if rating is less than 2.5 and double it
if (( $(awk 'BEGIN {print ("'$rating'" < 2.5)}') )); then
    rating=$(awk 'BEGIN {print "'$rating'" * 2}')
fi

# Prompt user for comments
read -p "Please provide your comments (Press Enter to skip): " comments

# Send feedback via email
echo -e "Name: $name\nRating: $rating\nComments: $comments" | mail -s "Training Feedback" -a "From: your_email@example.com" your_email@example.com

# Thank you message
echo "Thank you, $name, for participating in the training! Your feedback has been submitted."
----------------------------------------------------------------
better_compare(){
    echo -e "give me some sunshine :filename: \c"
    read -r filename
    read -n1 -p "Press any key to continue..."

    # Ask the user for source query type
    echo -e "\nChoose the source query type:"
    echo "1. Hive"
    echo "2. Oracle"
    echo "3. File Path"
    read -r -p "Enter your choice: " source_query_type

    case $source_query_type in
        1) 
            source_query_command="bline --outputformat=csv2 -f source_query.hql > source_data.csv"
            ;;
        2) 
            source_query_command="m_db unload 'source_query' > source_data.csv"
            ;;
        3) 
            read -r -p "Enter the file path for the source query: " source_query_path
            source_query_command="cat '$source_query_path' > source_data.csv"
            ;;
        *)
            echo "Invalid choice for source query type. Exiting."
            exit 1
            ;;
    esac

    # Ask the user for target query type
    echo -e "\nChoose the target query type:"
    echo "1. Hive"
    echo "2. Oracle"
    echo "3. File Path"
    read -r -p "Enter your choice: " target_query_type

    case $target_query_type in
        1) 
            target_query_command="bline --outputformat=csv2 -f target_query.hql > target_data.csv"
            ;;
        2) 
            target_query_command="m_db unload 'target_query' > target_data.csv"
            ;;
        3) 
            read -r -p "Enter the file path for the target query: " target_query_path
            target_query_command="cat '$target_query_path' > target_data.csv"
            ;;
        *)
            echo "Invalid choice for target query type. Exiting."
            exit 1
            ;;
    esac

    # Execute the source and target query commands
    eval "$source_query_command"
    eval "$target_query_command"

    # Call the second part of the function for comparison and HTML report creation
    run_query_create_evidence_part2
}
-------------------------------------------------------------------------
better_compare(){
    echo -e "give me some sunshine :filename: \c"
    read -r filename
    read -n1 -p "Press any key to continue..."

    # Ask the user for source query type
    echo -e "\nChoose the source query type:"
    echo "1. Oracle"
    echo "2. Hive"
    echo "3. File Path"
    read -r -p "Enter your choice: " source_query_type

    case $source_query_type in
        1) 
            echo "Enter your Oracle source query.Press Cntrl + D after writing the query."
            cat > source_query.hql
            ;;
        2) 
            echo "Enter your Hive source query.Press Cntrl + D after writing the query."
            cat > source_query.hql
            ;;
        3) 
            read -r -p "Enter the file path for the source query: " source_query_path
            cat "$source_query_path" > source_query.hql
            ;;
        *)
            echo "Invalid choice. Exiting."
            exit 1
            ;;
    esac

    # Ask the user for target query type
    echo -e "\nChoose the target query type:"
    echo "1. Oracle"
    echo "2. Hive"
    echo "3. File Path"
    read -r -p "Enter your choice: " target_query_type

    case $target_query_type in
        1) 
            echo "Enter your Oracle target query.Press Cntrl + D after writing the query."
            cat > target_query.hql
            ;;
        2) 
            echo "Enter your Hive target query.Press Cntrl + D after writing the query."
            cat > target_query.hql
            ;;
        3) 
            read -r -p "Enter the file path for the target query: " target_query_path
            cat "$target_query_path" > target_query.hql
            ;;
        *)
            echo "Invalid choice. Exiting."
            exit 1
            ;;
    esac

    run_query_create_evidence
}
--------------------------------------------------------------------

SELECT
  CASE
    WHEN SUM(CASE WHEN element = 1 THEN 1 ELSE 0 END) >= 9 THEN 'customer is adverse'
    ELSE 'customer is not adverse'
  END AS customer_status
FROM
  your_table_name
LATERAL VIEW EXPLODE(numbers_array) exploded_table AS element;
-------------------------------------------------------------------
#!/bin/bash

# Input file path
input_file="input.csv"

# Process the input file and generate the output
while IFS='|' read -r customer_id shopping_data; do
    # Count the number of zero shopping months using awk
    zero_months_count=$(echo "$shopping_data" | awk -F',' '{count=0; for(i=1;i<=NF;i++) if($i==0) count++; print count}')
    
    # Determine if the number of zero shopping months is at least 9
    if [ "$zero_months_count" -ge 9 ]; then
        has_at_least_nine_zeros=1
        additional_column_value=1331
    else
        has_at_least_nine_zeros=0
        additional_column_value=0
    fi
    
    # Output format: customer_id|number_of_zero_shopping_months|has_at_least_nine_zeros|additional_column_value
    echo "$customer_id|$zero_months_count|$has_at_least_nine_zeros|$additional_column_value"
done < "$input_file"
--------------------------------------------------------------------
#!/bin/bash

# Check if the user provided a file containing hostnames
if [ -z "$1" ]; then
    echo "Please provide a file containing hostnames."
    exit 1
fi

# Read hostnames from the provided file and resolve them to IP addresses
while IFS= read -r hostname; do
    ip=$(nslookup "$hostname" | awk '/^Address: / { print $2 }')
    echo "Hostname: $hostname, IP Address: $ip"
done < "$1"
--------+------------

hadoop fs -ls /path/to/your/hdfs/directory | grep -E '^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}$' | sort -t ' ' -k 1,2,3 -r | head -1

------------------------------
hadoop fs -ls /path/to/hdfs/directory | awk '{print $6, $7, $8}' | sort -k1,1n -k2,2M -k3,3n | tail -n 1 | awk '{print $3}'

---------------------------
#!/bin/bash

# Record start time
start_time=$(date "+%Y-%m-%d %H:%M:%S")

# Your existing shell script commands here
# For example:
# command1
# command2
# ...

# Record end time
end_time=$(date "+%Y-%m-%d %H:%M:%S")

# Find files generated between start and end time
files_to_zip=$(find /path/to/your/directory -type f -newermt "$start_time" ! -newermt "$end_time")

# Create a variable with newline characters replaced by spaces
files_to_be_removed=$(echo "$files_to_zip" | tr '\n' ' ')

# Zip the files
zip -n "abc$USER.zip" $files_to_zip

# Store filename and line count in /tmp/downstream_files_count.txt
for file in $files_to_be_removed; do
    line_count=$(wc -l < "$file")
    echo "Filename: $file, Line Count: $line_count" >> /tmp/downstream_files_count.txt
done

# Delete the files generated in the given time period
rm $files_to_be_removed
# Count the number of files
file_count=$(echo "$files_to_zip" | wc -l)

# Zip the files
zip -n "abc$USER.zip" $files_to_zip

# Store filename and line count in /tmp/downstream_files_count.txt
echo "Number of files generated in the given time period: $file_count" > /tmp/downstream_files_count.txt

for file in $files_to_be_removed; do
    basename_file=$(basename "$file")
    line_count=$(wc -l < "$file")
    echo "Filename: $basename_file, Line Count: $line_count" >> /tmp/downstream_files_count.txt
done
------------------------------------------------------------------------
from itertools import product

# Define the possible values for each variable
New_Marker_values = [0, 1]
Code_values = ['0762', '4208', '4199', '9876', '6546', '2354', '1295']
Limit_values = [-1000, -1001, -1002, -1003, -1004, -1005, -1006, -1007, -1008, -1009, -2000]

# Generate the Cartesian product of the possible values
sample_space = list(product(New_Marker_values, Code_values, Limit_values))

# Print all scenarios
for idx, scenario in enumerate(sample_space, start=1):
    New_Marker, Code, Limit = scenario
    print(f"{idx}. New_Marker={New_Marker}, Code={Code}, Limit={Limit}")


---------------------------
number=12.65

if [[ $number == *.* ]]; then
    integer_part=$(echo "$number" | awk -F'.' '{print $1}')
    decimal_part=$(echo "$number" | awk -F'.' '{print $2}')
else
    integer_part=$number
    decimal_part=""
fi
